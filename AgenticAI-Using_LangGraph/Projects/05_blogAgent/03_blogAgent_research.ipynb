{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a0873b",
   "metadata": {},
   "source": [
    "### **Load Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2f3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammadumerkhan/Gen-Agentic-AI-Tutorials/.venv/lib/python3.12/site-packages/langchain_tavily/tavily_research.py:97: UserWarning: Field name \"output_schema\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n",
      "/home/muhammadumerkhan/Gen-Agentic-AI-Tutorials/.venv/lib/python3.12/site-packages/langchain_tavily/tavily_research.py:97: UserWarning: Field name \"stream\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator, warnings\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, List, Annotated, Literal, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from IPython.display import Markdown, display, Image\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from prompt import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d69812",
   "metadata": {},
   "source": [
    "- **Test LLM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e238aace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "llm.invoke(\"hello\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c567e5",
   "metadata": {},
   "source": [
    "#### **Define Schemas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751b8ad",
   "metadata": {},
   "source": [
    "##### **Task**\n",
    "- **ID:** Task ID\n",
    "- **Title:** Title of the task\n",
    "- **Goal:** One sentence describing what the reader should be able to do/understand after this section.\n",
    "- **Bullets:** 3–5 concrete, non-overlapping subpoints to cover in this section.\n",
    "- **Target Words:** Target word count for this section (120–450).\n",
    "- **Tags:** List of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb28a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int = Field(..., description=\"ID of the task\")\n",
    "    title: str = Field(..., description=\"Title of the task\")\n",
    "    \n",
    "    goal: str = Field(\n",
    "        ..., \n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\"\n",
    "    )\n",
    "\n",
    "    bullets: List[str] = Field(\n",
    "        ..., min_length=3, max_length=5,\n",
    "        description=\"3–5 concrete, non-overlapping subpoints to cover in this section.\"\n",
    "    )\n",
    "\n",
    "    target_words: int  = Field(\n",
    "        ...,\n",
    "        description=\"Target word count for this section (120–450).\"\n",
    "    )\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8232ab",
   "metadata": {},
   "source": [
    "##### **Plan**\n",
    "- **Blog Title:** Title of the blog post\n",
    "- **Audience:** Who the blog is for.\n",
    "- **Tone:** Writing tone of the blog (e.g., practical, crisp).\n",
    "- **Blog Kind:** What kind of blog is this?\n",
    "- **Constraints:** List of constraints\n",
    "- **Tasks:** List of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4573f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str = Field(..., description=\"Title of the blog\")\n",
    "    audience: str = Field(..., description=\"Who the blog is for.\")\n",
    "    tone: str = Field(..., description=\"Writing tone of the blog (e.g., practical, crisp).\")\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task] = Field(..., description=\"List of tasks\", min_length=5, max_length=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4f101",
   "metadata": {},
   "source": [
    "##### **Evidence Item**\n",
    "- **Title:** Title of the evidence item\n",
    "- **URL:** URL of the evidence item\n",
    "- **Published At:** Published date of the evidence item\n",
    "- **Snippet:** Summary of the evidence item\n",
    "- **Source:** Source of the evidence item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fe966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceItem(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the evidence item\")\n",
    "    url: str = Field(..., description=\"URL of the evidence item\")\n",
    "    snippet: Optional[str] = Field(\n",
    "        ..., \n",
    "        description=\"Snippet of the evidence item\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c387fd",
   "metadata": {},
   "source": [
    "##### **Route Decision**\n",
    "- **Need Research:** Whether the task requires research\n",
    "- **Mode:** Mode of the task (open_book, hybrid, closed_book)\n",
    "- **Queries:** List of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1018dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteDecision(BaseModel):\n",
    "    need_research: bool = Field(\n",
    "        ..., \n",
    "        description=\"Whether the blog needs research (True/False).\"\n",
    "    )\n",
    "    \n",
    "    mode: Literal[\"open_book\", \"hybrid\", \"closed_book\"] = Field(\n",
    "        ..., \n",
    "        description=\"Mode of the blog (open_book/hybrid/closed_book).\"\n",
    "    )\n",
    "    \n",
    "    queries: List[str] = Field(\n",
    "        ..., \n",
    "        description=\"Queries for the blog.\",\n",
    "        default_factory=list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656a1db",
   "metadata": {},
   "source": [
    "##### **Evidence Pack**\n",
    "- **Evidence:** List of evidence items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b9eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(\n",
    "        description=\"List of evidence items\",\n",
    "        default_factory=list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596274d8",
   "metadata": {},
   "source": [
    "#### **Define State Model**\n",
    "- Shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47129509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str                                          # topic of the blog\n",
    "\n",
    "    # routing / research\n",
    "    mode: str                                           # mode of the blog\n",
    "    need_research: bool                                 # whether the blog needs research\n",
    "    queries: List[str]                                  # queries for the blog\n",
    "\n",
    "    # evidence\n",
    "    evidence: List[EvidencePack]                        # evidence for the blog\n",
    "    plan: Optional[Plan]                                # plan for the blog\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[str], operator.add]        # reducer: results from workers get concatenated automatically\n",
    "    final: str                                          # final blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473ec03",
   "metadata": {},
   "source": [
    "## **Nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f096f",
   "metadata": {},
   "source": [
    "- **Router:** Given a topic, decide whether the blog needs research or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb01293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state: State) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Router node for the blog agent.\n",
    "        - Given a topic, decide whether the blog needs research or not.\n",
    "        - If the blog needs research, decide the mode of the blog and the queries for the blog.\n",
    "    \"\"\"\n",
    "\n",
    "    topic = state['topic']\n",
    "    decider = llm.with_structured_output(RouteDecision).invoke([\n",
    "        SystemMessage(content=ROUTER_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"Topic: {topic}\")\n",
    "    ])\n",
    "\n",
    "    return { \n",
    "        \"need_research\": decider.need_research,\n",
    "        \"mode\": decider.mode,\n",
    "        \"queries\": decider.queries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95776bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = router_node({\n",
    "    \"topic\": \"Machine Learning\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43fdd592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'need_research': True,\n",
       " 'mode': 'hybrid',\n",
       " 'queries': ['latest advancements in machine learning algorithms 2024',\n",
       "  'top machine learning frameworks and tools in 2024',\n",
       "  'case studies of successful machine learning implementation in industry 2024',\n",
       "  'machine learning model interpretability techniques 2024',\n",
       "  'impact of recent hardware improvements on machine learning training 2024']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f00bdf",
   "metadata": {},
   "source": [
    "##### **Route Next:** Given a state, decide whether to route to the research node or the orchestrator node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bcb2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_next(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Router function for the blog agent.\n",
    "        - Given a topic, decide whether the blog needs research or not.\n",
    "        - If the blog needs research, decide the mode of the blog and the queries for the blog.\n",
    "    \"\"\"\n",
    "    return \"research\" if state[\"need_research\"] else \"orchestrate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de483d7",
   "metadata": {},
   "source": [
    "##### **Tavily Search:** Given a query, search for evidence\n",
    "- **Query:** Query to search for evidence.\n",
    "- **Max Results:** Maximum number of results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c10580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "\n",
    "    search = TavilySearch(max_results=max_results)\n",
    "    results = search.invoke({ \"query\": query })\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results['results'] or []:\n",
    "        normalized.append({\n",
    "            \"title\": r.get(\"title\") or \"\",\n",
    "            \"url\": r.get(\"url\") or \"\",\n",
    "            \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "        })\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f746594",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tavily_search = _tavily_search(\"latest machine learning algorithms 2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f505bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary: This blog highlights ten crucial Machine Learning algorithms to know in 2024, including linear regression, decision trees,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tavily_search[0]['snippet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81a158e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Research node for the blog agent.\n",
    "        - Given queries, search for evidence.\n",
    "        - Extract evidence from the search results.\n",
    "        - Return evidence.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = state.get(\"queries\", [])\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return { \"evidence\": [] }\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack).invoke([\n",
    "        SystemMessage(content=RESEARCH_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"Raw Results: \\n{raw_results}\")\n",
    "    ])\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in extractor.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return { \"evidence\": list(dedup.values()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "720115e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latest advancements in machine learning algorithms 2024',\n",
       " 'top machine learning frameworks and tools in 2024',\n",
       " 'case studies of successful machine learning implementation in industry 2024']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions['queries'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed49b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = research_node({\n",
    "    \"queries\": decisions['queries'][:3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fc5ed3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI and Machine Learning Trends in 2024 - Dataversity'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results['evidence'][0].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9691dfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([e.model_dump() for e in query_results['evidence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8cc743",
   "metadata": {},
   "source": [
    "- **Orchestrator:** Given a topic, generate 5-7 sections on the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b53c2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "    \"\"\" \n",
    "    Orchestrator function for the blog agent.\n",
    "        - Given a topic, mode, and evidence, generate a blog plan.\n",
    "        - Generate:\n",
    "            - blog title\n",
    "            - audience\n",
    "            - tone\n",
    "            - tasks\n",
    "    \"\"\"\n",
    "\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    plan = planner.invoke([\n",
    "        SystemMessage(content=ORCH_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=(\n",
    "            f\"Topic: {state['topic']}\\n\"\n",
    "            f\"Mode: {mode}\\n\\n\"\n",
    "            f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "            f\"{[e.model_dump() for e in evidence][:16]}\"\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    return { \"plan\":plan }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdc219b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_results['evidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8535caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = orchestrator({\n",
    "    \"topic\": \"Machine Learning\",\n",
    "    \"mode\": decisions['mode'],\n",
    "    \"evidence\": query_results['evidence'] or [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c66801f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plans['plan'].tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44076a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### **Overview of Machine Learning Trends in 2024**\n",
       "\n",
       "**Goal:** Provide a high-level summary of the key machine learning trends shaping development in 2024.\n",
       "\n",
       "**Bullets:**\n",
       "  - Identify the shift towards cloud-based ML and edge AI deployments as dominant trends.\n",
       "  - Summarize the increased focus on data-centric frameworks and reduced data dependency techniques.\n",
       "  - Mention advancements in NLP algorithms improving machine comprehension accuracy.\n",
       "  - Highlight the rise of meta-learning, prompt engineering, and self-supervised approaches.\n",
       "  - Explain the growing importance of AI ethics and governance in ML workflows.\n",
       "\n",
       "**Target Words:** 400  \n",
       "\n",
       "**Tags:** ['trends', 'overview', '2024'] \n",
       "\n",
       "**Requires Research:** True \n",
       "\n",
       "**Requires Citations:** True   \n",
       "\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### **Emerging Machine Learning Algorithms and Techniques**\n",
       "\n",
       "**Goal:** Detail the new and evolving algorithms and methodologies gaining traction in 2024 ML projects.\n",
       "\n",
       "**Bullets:**\n",
       "  - Describe Generative Adversarial Networks (GANs) and their use in image generation and data augmentation.\n",
       "  - Outline meta-learning and its utility in reducing training data requirements and increasing adaptability.\n",
       "  - Discuss self-supervised learning's role in leveraging unlabeled data efficiently.\n",
       "  - Summarize advancements in automated machine learning (AutoML) and its impact on model development workload.\n",
       "  - Mention practical applications of prompt engineering and model fine-tuning in real-world scenarios.\n",
       "\n",
       "**Target Words:** 450  \n",
       "\n",
       "**Tags:** ['algorithms', 'techniques', '2024'] \n",
       "\n",
       "**Requires Research:** True \n",
       "\n",
       "**Requires Citations:** True   \n",
       "\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### **Top Machine Learning Frameworks and Tools for Developers in 2024**\n",
       "\n",
       "**Goal:** Help developers understand the leading ML frameworks and libraries relevant for 2024 development projects.\n",
       "\n",
       "**Bullets:**\n",
       "  - List prominent frameworks such as TensorFlow, PyTorch, Keras, and Scikit-learn and their distinctive features.\n",
       "  - Discuss PyTorch’s popularity among researchers and its TorchServe model deployment capabilities.\n",
       "  - Outline TensorFlow’s integration with Keras and support for diverse model types including vision and text.\n",
       "  - Explain lightweight and specialized tools gaining traction, including OpenCV for computer vision.\n",
       "  - Provide insights into tool selection criteria considering efficiency, community, and support.\n",
       "\n",
       "**Target Words:** 440  \n",
       "\n",
       "**Tags:** ['tools', 'frameworks', '2024', 'developer'] \n",
       "\n",
       "**Requires Research:** True \n",
       "\n",
       "**Requires Citations:** True   \n",
       "\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### **Industry Applications: Case Studies of Successful ML Implementations**\n",
       "\n",
       "**Goal:** Illustrate how leading industries are applying machine learning with tangible business value in 2024.\n",
       "\n",
       "**Bullets:**\n",
       "  - Summarize JPMorgan Chase’s use of ML for risk management and fraud detection.\n",
       "  - Detail Amazon’s personalization strategies and inventory optimization using ML.\n",
       "  - Describe Siemens’ predictive maintenance and equipment failure prediction initiatives.\n",
       "  - Include manufacturing-specific cases of ML improving quality control, energy efficiency, and process optimization.\n",
       "  - Highlight warehouse automation successes driven by machine learning deployments.\n",
       "\n",
       "**Target Words:** 430  \n",
       "\n",
       "**Tags:** ['industry', 'case studies', 'applications'] \n",
       "\n",
       "**Requires Research:** True \n",
       "\n",
       "**Requires Citations:** True   \n",
       "\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### **Challenges and Considerations in Modern Machine Learning Deployments**\n",
       "\n",
       "**Goal:** Discuss common hurdles, security considerations, and best practices for deploying ML in 2024.\n",
       "\n",
       "**Bullets:**\n",
       "  - Examine data privacy and ethics issues mandatory for compliant ML solutions.\n",
       "  - Explore challenges in federated learning environments, including data heterogeneity.\n",
       "  - Address debugging and observability techniques critical for production ML models.\n",
       "  - Highlight performance and cost implications of cloud versus edge AI deployment.\n",
       "  - Discuss handling edge cases and failure modes for robust model operation under real-world conditions.\n",
       "\n",
       "**Target Words:** 420  \n",
       "\n",
       "**Tags:** ['challenges', 'security', 'performance', 'debugging'] \n",
       "\n",
       "**Requires Research:** True \n",
       "\n",
       "**Requires Citations:** True   \n",
       "\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in plans[\"plan\"].tasks:\n",
    "    bullets_md = \"\\n\".join(f\"  - {b}\" for b in task.bullets)\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "#### **{task.title}**\n",
    "\n",
    "**Goal:** {task.goal}\n",
    "\n",
    "**Bullets:**\n",
    "{bullets_md}\n",
    "\n",
    "**Target Words:** {task.target_words}  \\n\n",
    "**Tags:** {task.tags or 'None'} \\n\n",
    "**Requires Research:** {task.requires_research} \\n\n",
    "**Requires Citations:** {task.requires_citations}   \\n\n",
    "\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a37a6",
   "metadata": {},
   "source": [
    "- **Worker:** Given a task, generate a blog section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b94d1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan(**plans['plan'].model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63d1e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task(**plans['plan'].tasks[0].model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9f2317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvidenceItem(**query_results['evidence'][0].model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Worker function for the blog agent.\n",
    "        - Given a task, generate a blog section.\n",
    "    \"\"\"\n",
    "    \n",
    "    # payload contains what we sent\n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "    bullet_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url}\".strip() for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke([\n",
    "        SystemMessage(content=WORKER_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=(\n",
    "            f\"Blog Title: {blog_title}\\n\"\n",
    "            f\"Audience: {plan.audience}\\n\"\n",
    "            f\"Tone: {plan.tone}\\n\\n\"\n",
    "            f\"Blog Kind: {plan.blog_kind}\\n\"\n",
    "            f\"Constraints: {plan.constraints}\\n\"\n",
    "            f\"Topic: {topic}\\n\\n\"\n",
    "            f\"Mode: {mode}\\n\"\n",
    "            f\"Section Title: {task.title}\\n\"\n",
    "            f\"Goal: {task.goal}\\n\\n\"\n",
    "            f\"Target Words: {task.target_words}\\n\\n\"\n",
    "            f\"Tags: {\", \".join(task.tags or [])}\\n\\n\"\n",
    "            f\"requires_research: {\"Yes\" if task.requires_research else \"No\"}\\n\\n\"\n",
    "            f\"requires_citations: {\"Yes\" if task.requires_research else \"No\"}\\n\\n\"\n",
    "            f\"requires_code: {\"Yes\" if task.requires_research else \"No\"}\\n\\n\"\n",
    "            f\"Evidence: {evidence_text}\\n\\n\"\n",
    "            f\"Bullet: {bullet_text}\\n\\n\"\n",
    "            f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "        ))\n",
    "    ]).content.strip()\n",
    "\n",
    "    return { \"sections\": [(task.id, section_md)] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e9e11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = worker({\n",
    "    \"task\": plans['plan'].tasks[0].model_dump(),\n",
    "    \"plan\": plans[\"plan\"].model_dump(),\n",
    "    \"topic\": \"Machine Learning\",\n",
    "    \"mode\": decisions['mode'],\n",
    "    \"evidence\": [e.model_dump() for e in query_results['evidence']],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "442e06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, md = section['sections'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e718c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Overview of Machine Learning Trends in 2024\n",
       "\n",
       "In 2024, machine learning (ML) development is markedly influenced by the increasing shift towards **cloud-based ML and edge AI deployments**. Cloud platforms provide scalable infrastructure, enabling rapid model training and deployment with reduced latency. Concurrently, edge AI is gaining traction for real-time inference on-device, essential for applications requiring low-latency responses, such as autonomous systems and IoT devices. This hybrid approach balances computational costs with performance needs, reflecting the industry's pivot towards distributed ML architectures ([Dataversity](https://www.dataversity.net/articles/ai-and-machine-learning-trends-in-2024/), [Encord](https://encord.com/blog/machine-learning-trends-statistics/)).\n",
       "\n",
       "A key focus this year centers on **data-centric AI frameworks** which prioritize improving data quality, annotation, and augmentation over solely scaling models. This shift emerges from the realization that many ML success stories rely heavily on optimized datasets rather than bigger models. Techniques reducing dependency on massive labeled datasets are also advancing, with innovations like semi-supervised learning and synthetic data generation lowering barriers to entry and accelerating development cycles ([Capicua](https://www.capicua.com/blog/machine-learning), [Kaggle](https://www.kaggle.com/general/543653)).\n",
       "\n",
       "Natural Language Processing (NLP) algorithms are undergoing significant improvements in 2024, pushing the boundaries of **machine comprehension accuracy**. Transformer-based architectures now incorporate more efficient fine-tuning methods and enhanced embeddings, delivering better context understanding and reasoning capabilities. These advancements enable more nuanced semantic search, summarization, and conversational AI systems that are closer to human-like language understanding ([LinkedIn](https://www.linkedin.com/pulse/top-emerging-trends-machine-learning-2024-nick-gupta-uaasc), [DEV Community](https://dev.to/mankavelda/10-machine-learning-algorithms-to-know-in-2024-1p8j)).\n",
       "\n",
       "Emerging learning paradigms such as **meta-learning, prompt engineering, and self-supervised learning** are shaping innovative workflows. Meta-learning techniques facilitate rapid adaptation of models to new tasks with limited data. Prompt engineering has become critical for optimizing large language models’ outputs without retraining. Self-supervised approaches exploit unlabeled data to learn feature representations effectively, making models less reliant on costly annotations ([Kaggle](https://www.kaggle.com/general/543653), [Medium](https://medium.com/@nemagan/top-machine-learning-frameworks-and-libraries-in-2024-5474cb10e039)).\n",
       "\n",
       "Lastly, the role of **AI ethics and governance** is increasingly vital in ML workflows. With growing regulatory scrutiny and public awareness, organizations are embedding fairness, transparency, and privacy-preserving mechanisms throughout the ML lifecycle. Tooling for bias detection, explainability, and compliance tracking is becoming mainstream, enabling responsible AI deployment that aligns with ethical standards and legal requirements ([Dataversity](https://www.dataversity.net/articles/ai-and-machine-learning-trends-in-2024/), [Aalpha](https://www.aalpha.net/articles/top-artificial-intelligence-frameworks-tools/)).\n",
       "\n",
       "```python\n",
       "# Example: Self-supervised learning using contrastive loss (PyTorch)\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "class ContrastiveLoss(nn.Module):\n",
       "    def __init__(self, margin=1.0):\n",
       "        super().__init__()\n",
       "        self.margin = margin\n",
       "\n",
       "    def forward(self, x1, x2, label):\n",
       "        distance = F.pairwise_distance(x1, x2)\n",
       "        loss = torch.mean(\n",
       "            label * torch.pow(distance, 2) +\n",
       "            (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
       "        )\n",
       "        return loss\n",
       "\n",
       "# Usage: optimize embeddings to bring positive pairs closer and push negatives away\n",
       "```\n",
       "\n",
       "By integrating these trends, developers can build more efficient, robust, and ethical ML solutions aligned with 2024’s evolving landscape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8074b",
   "metadata": {},
   "source": [
    "- **Fanout:** Distribute tasks to workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbfae18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\", \n",
    "            { \n",
    "                \"task\": task.model_dump(), \n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"], \n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state['evidence']],\n",
    "            })\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886e295",
   "metadata": {},
   "source": [
    "- **Reducer:** Given a list of blog sections, generate a final blog post, and save it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2ae88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(state: State) -> dict:\n",
    "    \"\"\" \n",
    "    Reducer function for the blog agent.\n",
    "        - Given a list of blog sections, generate a final blog post.\n",
    "        - The final blog post is saved to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_path = Path.cwd() / \"outputs\"\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    final_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # Save the final blog post\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    output_path = Path.cwd() / \"outputs\" / filename\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return { \"final\": final_md }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049e4e4",
   "metadata": {},
   "source": [
    "- **Graph Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f8fcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"router\", router_node)\n",
    "graph.add_node(\"researcher\", research_node)\n",
    "graph.add_node(\"orchestrator\", orchestrator)\n",
    "graph.add_node(\"worker\", worker)\n",
    "graph.add_node(\"reducer\", reducer)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, \"router\")\n",
    "graph.add_conditional_edges(\"router\", route_next, { \"research\" : \"researcher\", \"orchestrate\": \"orchestrator\" })\n",
    "graph.add_edge(\"researcher\", \"orchestrator\")\n",
    "graph.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "graph.add_edge(\"worker\", \"reducer\")\n",
    "graph.add_edge(\"reducer\", END)\n",
    "\n",
    "# Compile graph\n",
    "blog = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04670b",
   "metadata": {},
   "source": [
    "- **Graph Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f69dad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(blog.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d7cb5",
   "metadata": {},
   "source": [
    "##### **Run function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4d9e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(topic: str) -> dict:\n",
    "    \"\"\" \n",
    "    Run the blog agent.\n",
    "        - Given a topic, generate a blog post.\n",
    "        - The blog post is saved to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = blog.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"mode\": \"\",\n",
    "        \"need_research\": False,\n",
    "        \"queries\": [],\n",
    "        \"evidence\": [],\n",
    "        \"plan\": None,\n",
    "        \"sections\": [],\n",
    "        \"final\": \"\",\n",
    "    })\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e210580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opensource_llms = run(\"Write a blog on Open Source LLMs in 2026\")\n",
    "multimodal_llms = run(\"State of Multimodal LLMs in 2026\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f384ae32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# State of Multimodal LLMs in 2026\n",
       "\n",
       "## Overview of Multimodal LLMs in 2026\n",
       "\n",
       "Multimodal large language models (LLMs) in 2026 represent a significant evolution beyond traditional text-only models. These models integrate multiple data modalities—text, images, audio, and video—enabling comprehensive understanding and generation across diverse input types. This multimodality equips them to perform vision-language reasoning, audio transcription with contextual analysis, and even video summarization, bridging gaps between different sensory data sources ([Analytics Vidhhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/), [Ruh.ai](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026)).\n",
       "\n",
       "Among the current leaders, GLM-4.5V, Qwen2.5-VL-32B-Instruct, and Falcon 2 are recognized top performers. GLM-4.5V excels in high-fidelity visual question answering and image captioning tasks. Qwen2.5-VL-32B-Instruct integrates large-scale instruction tuning with expansive visual understanding, providing interactive multimodal dialogue capabilities. Falcon 2 offers efficient scaling and fine-grained multimodal alignment, effectively managing diverse input forms ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/), [TechTarget](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\n",
       "\n",
       "General capabilities of these models include reasoning across modalities by correlating textual context with visual or audio inputs, enabling tasks such as cross-modal retrieval, multimodal summarization, and grounded generation. They also support diverse inputs simultaneously, facilitating applications ranging from interactive chatbots that understand images and speech to assistive systems for content creation and accessibility ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends), [Hatchworks](https://hatchworks.com/blog/gen-ai/large-language-models-guide/)).\n",
       "\n",
       "The rapid advances in multimodal LLMs are driven by two key factors: the expansion of large, high-quality multimodal datasets and architectural innovations. Enlarged datasets now encompass paired image-text, audio-transcript, and video-caption collections that improve cross-modal alignment. At the same time, novel transformer architectures and training paradigms, including vision-language fusion layers and modality-specific adapters, enhance the models’ ability to integrate and reason across heterogeneous data ([AIMultiple](https://research.aimultiple.com/large-multimodal-models/), [SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\n",
       "\n",
       "Complementing proprietary systems, emerging open-source multimodal models have significantly improved accessibility for developers. Projects such as OpenFlamingo and LLaVA variants offer customizable, community-driven multimodal tools, lowering the barrier for research and application development. These open models emphasize modular design, enabling fine-tuning or extension over specialized data, thus fostering innovation in practical use cases from healthcare diagnostics to multimedia content generation ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models), [Ruh.ai](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026)).\n",
       "\n",
       "```python\n",
       "# Example: basic inference with a multimodal model (pseudo-code)\n",
       "model = load_multimodal_model(\"GLM-4.5V\")\n",
       "text_input = \"Describe the scene in this image.\"\n",
       "image_input = load_image(\"sample_photo.jpg\")\n",
       "output = model.infer(text=[text_input], images=[image_input])\n",
       "print(output)\n",
       "```\n",
       "\n",
       "In summary, multimodal LLMs in 2026 combine advanced architectures and large-scale multimodal data, enabling nuanced understanding across text, vision, audio, and video. Top models achieve state-of-the-art performance in integrated reasoning tasks, while open-source alternatives expand developer access and customization opportunities. This landscape continues to drive innovation across industries relying on rich, multimodal AI capabilities.\n",
       "\n",
       "## Major Model Architectures and Innovations in 2026\n",
       "\n",
       "Modern multimodal LLM architectures in 2026 blend transformer backbones with specialized modality encoders and fusion layers to handle diverse inputs such as text, images, video, and audio. Typically, modality-specific encoders extract high-level features (e.g., convolutional layers for images or waveform encoders for audio), which are then mapped into a unified embedding space. These embeddings feed into a shared transformer core equipped with fusion layers designed to learn cross-modal interactions effectively. This modular design enables scalable and flexible integration across modalities while maintaining strong contextual understanding ([Analytics Vidhhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/), [SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\n",
       "\n",
       "A notable trend gaining traction is the use of multimodal chains and agent-based architectures. These systems combine modular input processing with integrated reasoning components, enabling sequential and conditional handling of multimodal inputs. Agents can dynamically select which modalities to attend to and how to fuse information, resulting in more interpretable and flexible pipelines that adapt reasoning strategies depending on task demands ([Ruh AI](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026)).\n",
       "\n",
       "Parameter scaling remains pivotal in enhancing multimodal model capability, with state-of-the-art designs reaching hundreds of billions of parameters. However, efficiency is prioritized through advanced fine-tuning techniques such as adapter modules, low-rank adaptation (LoRA), and prompt tuning. These approaches reduce update footprint and resource consumption while achieving task-specific specialization ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends), [Medium](https://medium.com/@bergamo.gustavo/a-net-developers-guide-to-choosing-the-right-llm-architecture-in-2026-9ccf2f7bf65b)).\n",
       "\n",
       "Innovations in dynamic modality attention mechanisms have greatly improved generalization by enabling the model to weigh and attend to modalities selectively based on input context. This reduces noise from irrelevant modalities and strengthens cross-modal transfer learning, where knowledge learned in one modality boosts performance in others. Such transfer enables efficient adaptation in low-data regimes and enhances robustness across diverse tasks ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\n",
       "\n",
       "Recent architecture-focused evaluations highlight trade-offs in deploying large multimodal models locally versus in the cloud. Local deployment favors privacy and lower latency, especially with emerging lightweight architectures and pruning. Cloud-based solutions still dominate when requiring massive compute resources for large-scale training and real-time multimodal inference pipelines ([.NET Guide](https://medium.com/@bergamo.gustavo/a-net-developers-guide-to-choosing-the-right-llm-architecture-in-2026-9ccf2f7bf65b), [AIMultiple](https://research.aimultiple.com/large-multimodal-models/)).\n",
       "\n",
       "```python\n",
       "# Example: Modular multimodal fusion layer in PyTorch-like pseudocode\n",
       "class FusionLayer(nn.Module):\n",
       "    def __init__(self, hidden_size):\n",
       "        super().__init__()\n",
       "        self.cross_modal_attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
       "        self.norm = nn.LayerNorm(hidden_size)\n",
       "\n",
       "    def forward(self, text_embeds, image_embeds):\n",
       "        # Concatenate sequences for cross-modal attention\n",
       "        combined = torch.cat([text_embeds, image_embeds], dim=0)\n",
       "        attn_output, _ = self.cross_modal_attention(combined, combined, combined)\n",
       "        return self.norm(attn_output)\n",
       "```\n",
       "\n",
       "This layered, attentive fusion approach serves as a foundation for many top multimodal models powering diverse applications in 2026.\n",
       "\n",
       "---\n",
       "\n",
       "Sources:  \n",
       "[Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)  \n",
       "[SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)  \n",
       "[Ruh AI](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026)  \n",
       "[Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)  \n",
       "[Medium .NET Guide](https://medium.com/@bergamo.gustavo/a-net-developers-guide-to-choosing-the-right-llm-architecture-in-2026-9ccf2f7bf65b)  \n",
       "[AIMultiple](https://research.aimultiple.com/large-multimodal-models/)\n",
       "\n",
       "## Performance Benchmarks and Evaluation Techniques\n",
       "\n",
       "Multimodal large language models (LLMs) in 2026 are rigorously evaluated using a combination of benchmark datasets and diverse task-specific metrics to capture their effectiveness across modalities. Two prominent benchmark datasets widely adopted for multimodal training and evaluation are **LAION-5B**, a vast dataset comprising over 5 billion image-text pairs enabling broad visual-language alignment, and the **LAMM-Dataset**, which integrates image, text, audio, and video data, facilitating comprehensive multimodal model assessment ([Source](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\n",
       "\n",
       "Key evaluation tasks reflect real-world and research challenges, including:\n",
       "\n",
       "- **Visual Question Answering (VQA):** Models must interpret images and generate accurate textual answers.\n",
       "- **Audio-Text Comprehension:** Assessing the ability to align spoken language inputs with textual or visual data.\n",
       "- **Video Understanding:** Evaluating dynamic scene recognition and narrative extraction from video streams.\n",
       "\n",
       "These varied tasks highlight different model competencies, often revealing specialized strengths. For example, several state-of-the-art multimodal LLMs outperform traditional LLMs in **financial reasoning** tasks by effectively integrating textual data with financial charts or reports. Conversely, in **clinical applications**, multimodal models combining imaging and patient record text show remarkable promise but still lag behind domain-specialized systems in nuanced diagnosis ([Source](https://jamanetwork.com/journals/jama/fullarticle/2816270)).\n",
       "\n",
       "Evaluation metrics extend beyond accuracy to incorporate:\n",
       "\n",
       "- **Inference Speed:** Measuring real-time processing capabilities essential for deployment.\n",
       "- **Multimodal Reasoning Accuracy:** The precision of synthesizing inputs across modalities to answer complex queries.\n",
       "- **Real-World Task Adaptability:** How well models generalize to use cases like autonomous assistance or multimedia content generation.\n",
       "\n",
       "Standardizing benchmarks remains a key challenge due to heterogeneous data types, annotation inconsistencies, and varying evaluation protocols. Ongoing research increasingly emphasizes unifying evaluation frameworks and developing modality-agnostic metrics to close performance gaps. Collaborative efforts toward benchmark interoperability and the inclusion of underrepresented modalities aim to foster a more holistic and comparable model evaluation landscape ([Source](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\n",
       "\n",
       "Below is a minimal example illustrating how you might programmatically measure inference latency and accuracy for a multimodal VQA task in Python pseudocode:\n",
       "\n",
       "```python\n",
       "import time\n",
       "\n",
       "def evaluate_model(model, dataset):\n",
       "    total_time = 0\n",
       "    correct_answers = 0\n",
       "    for image, question, ground_truth in dataset:\n",
       "        start = time.time()\n",
       "        prediction = model.predict(image=image, text=question)\n",
       "        total_time += time.time() - start\n",
       "        if prediction == ground_truth:\n",
       "            correct_answers += 1\n",
       "    accuracy = correct_answers / len(dataset)\n",
       "    avg_latency = total_time / len(dataset)\n",
       "    return accuracy, avg_latency\n",
       "\n",
       "# Example usage with a dummy dataset and model instance\n",
       "# accuracy, latency = evaluate_model(my_multimodal_model, vqa_test_set)\n",
       "# print(f\"Accuracy: {accuracy:.2f}, Avg Inference Time: {latency:.3f} seconds\")\n",
       "```\n",
       "\n",
       "This approach underscores critical benchmarking dimensions necessary to assess the performance of multimodal LLMs comprehensively.\n",
       "\n",
       "---\n",
       "\n",
       "Overall, the landscape in 2026 reflects maturing methodologies that balance diverse modalities, task complexities, and operational constraints, driving a more robust evaluation ecosystem for multimodal LLMs.\n",
       "\n",
       "[Source](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/) | [Source](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025) | [Source](https://jamanetwork.com/journals/jama/fullarticle/2816270)\n",
       "\n",
       "## Emerging Use Cases and Industry Adoption\n",
       "\n",
       "Multimodal large language models (LLMs) have rapidly expanded their footprint across various applications in 2026. Popular use cases now prominently include virtual assistants that comprehend and generate responses from combined text, image, and audio inputs. Medical imaging analysis is a critical domain where multimodal LLMs help detect anomalies by interpreting radiology images alongside patient histories, significantly improving diagnostic accuracy. Additionally, multimedia document understanding—encompassing video content summarization, multi-format document ingestion, and cross-modal search—has become a core capability driving enterprise interest ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/), [The NineHertz](https://theninehertz.com/blog/multimodal-ai-use-cases)).\n",
       "\n",
       "Enterprise adoption is propelled by advancements in accuracy, reduced inference latency, and the ability to process diverse input modalities simultaneously. Organizations report tangible improvements in workflow automation and decision support, especially where contextual understanding from multiple data types is required. Cloud providers and AI platforms now offer optimized APIs tailored to multimodal workloads, enabling firms to integrate these models without extensive infrastructure overhaul ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends), [SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\n",
       "\n",
       "Healthcare, finance, and media production are among the industries most impacted by this shift. In healthcare, multimodal LLMs assist in clinical decision-making through combined analysis of imaging, text notes, and genetic data. Financial services leverage these models for risk assessment by correlating numerical data trends with textual news and regulatory filings. Media companies deploy multimodal LLMs for automated content generation, rights management, and enhanced user engagement through interactive multimedia experiences ([JAMA Network](https://jamanetwork.com/journals/jama/fullarticle/2816270), [TechTarget](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\n",
       "\n",
       "For developers, integration considerations have evolved. API accessibility now emphasizes support for multimodal inputs with unified endpoints. Deployment options range from fully managed cloud services to edge-optimized containers for latency-sensitive applications. Data annotation remains key, with specialized multimodal annotation tools gaining prominence to label complex datasets involving images, audio, and text simultaneously, essential for fine-tuning and domain adaptation ([Taskmonk](https://www.taskmonk.ai/blogs/top-multimodal-annotation-tools-2026), [GitHub Jinbo0906](https://github.com/jinbo0906/Awesome-MLLM-Datasets)).\n",
       "\n",
       "Overall, multimodal LLMs enable novel product features such as context-aware virtual support, enriched content summarization, and predictive analytics that integrate visual and textual cues. These capabilities improve AI-driven workflows at scale by reducing manual data synthesis and enabling deeper insights across heterogeneous data sources, marking a significant advancement over unimodal language models ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models), [Citrusbug](https://citrusbug.com/blog/llm-use-cases/)).\n",
       "\n",
       "```python\n",
       "# Example: Simple API call structure leveraging a multimodal LLM for text+image input\n",
       "import requests\n",
       "\n",
       "endpoint = \"https://api.multimodalllm.example.com/v1/infer\"\n",
       "headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
       "data = {\n",
       "    \"text\": \"Find anomalies in the attached chest X-ray image.\",\n",
       "    \"image_url\": \"https://example.com/images/chest_xray_12345.png\"\n",
       "}\n",
       "\n",
       "response = requests.post(endpoint, json=data, headers=headers)\n",
       "print(response.json())\n",
       "```\n",
       "This pattern illustrates integration simplicity as vendors converge on unified multimodal input APIs, enabling rapid adoption and innovation.\n",
       "\n",
       "## Ecosystem Tools: Annotation, Datasets, and Open Source Support\n",
       "\n",
       "The multimodal LLM ecosystem in 2026 has matured with robust tools, expansive datasets, and an active open-source community driving innovation and accessibility.\n",
       "\n",
       "**Annotation Tools**  \n",
       "Top multimodal data annotation platforms now comprehensively support labeling across text, images, audio, video, and even 3D modalities. Leading tools like Labelbox, Supervisely, and CVAT enable expert collaboration through features such as real-time review, multi-user workflows, and integrated model-assisted labeling. These tools streamline workflows for diverse data types, enhancing dataset quality crucial for training advanced multimodal models ([Taskmonk](https://www.taskmonk.ai/blogs/top-multimodal-annotation-tools-2026)).\n",
       "\n",
       "**Large-Scale Multimodal Datasets**  \n",
       "Several large-scale datasets continue to fuel the development of state-of-the-art models. LAION-5B remains a cornerstone dataset with billions of image-text pairs, providing rich diversity for vision-language tasks. ArXivCap, a specialized dataset compiling scientific paper images and corresponding captions, has become a benchmark for domain-specific multimodal research. Repositories like jinbo0906’s GitHub collection curate and update over 50 multimodal datasets, covering modalities including audio-captioning and video-question answering, catering to wide research needs ([AIMultiple](https://research.aimultiple.com/datasets-for-ml/), [GitHub](https://github.com/jinbo0906/Awesome-MLLM-Datasets)).\n",
       "\n",
       "**Open-Source Multimodal Models**  \n",
       "Open-source projects play a crucial role in democratizing multimodal AI. Models such as OpenFlamingo, MM-VLM, and LLaVA provide flexible architectures supporting multiple input types and enabling fine-tuning for custom applications. These repositories come with pretrained checkpoints and modular design, fostering rapid experimentation and research. The fastest open-source models now offer inference speeds competitive with commercial offerings, lowering latency barriers for deployment in real-world scenarios ([SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025), [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\n",
       "\n",
       "**Benchmarking and Frameworks**  \n",
       "To standardize evaluation, benchmarking suites like MM-Bench and HELM integrate multimodal task tracks, allowing comprehensive performance comparisons across vision, audio, and text tasks. Frameworks such as Hugging Face’s Transformers and MMF have extended native support for multimodal pipelines, simplifying prototyping with end-to-end pipelines and pretrained multimodal datasets as first-class citizens. These tools accelerate development cycles by combining ease of use with rigorous benchmarking capabilities ([Ruh.ai](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026)).\n",
       "\n",
       "**Lowering Entry Barriers**  \n",
       "Together, these ecosystem components significantly reduce friction for developers and researchers entering multimodal AI. High-quality annotated datasets and collaborative tooling minimize dataset creation overhead. Open-source models reduce dependency on expensive proprietary APIs. Accessible benchmarking and modular frameworks enable rapid iteration. This vibrant ecosystem fosters innovation, accelerating the pace at which multimodal LLMs evolve and find new applications across industries ([Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\n",
       "\n",
       "```python\n",
       "# Example: Load an open-source multimodal model using Hugging Face Transformers\n",
       "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
       "\n",
       "processor = AutoProcessor.from_pretrained(\"openflamingo/OpenFlamingo\")\n",
       "model = AutoModelForVision2Seq.from_pretrained(\"openflamingo/OpenFlamingo\")\n",
       "\n",
       "# Prepare multimodal inputs (image + text)\n",
       "images = [...]  # List of input images (PIL.Image)\n",
       "text_prompts = [\"Describe the image\"]\n",
       "\n",
       "inputs = processor(images=images, text=text_prompts, return_tensors=\"pt\", padding=True)\n",
       "\n",
       "outputs = model.generate(**inputs)\n",
       "print(processor.decode(outputs[0], skip_special_tokens=True))\n",
       "```\n",
       "\n",
       "This ecosystem momentum sets a solid foundation for continued advancements in multimodal LLM capabilities throughout 2026 and beyond.\n",
       "\n",
       "## Challenges, Limitations, and Future Directions\n",
       "\n",
       "Multimodal large language models (LLMs) in 2026 have achieved impressive capabilities but still face several key limitations. One major challenge is **modality imbalance**, where models excel in one modality (e.g., text) but underperform on others such as vision or audio. This causes inconsistent outputs, especially in complex, multimodal reasoning tasks where integration across formats is crucial. Additionally, **reasoning gaps** persist for nuanced, context-rich scenarios, limiting the ability to draw sophisticated inferences that combine semantic, temporal, and spatial information effectively. Generalization beyond benchmark data remains an open issue, as models often fail to robustly handle out-of-distribution inputs or rare modality combinations ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/), [AI Multiple](https://research.aimultiple.com/large-multimodal-models/)).\n",
       "\n",
       "In real-world deployments, typical **failure modes** include errors in semantic segmentation and misalignment between modalities when inputs are noisy or incomplete. For instance, multimodal inputs corrupted by environmental noise or poor lighting degrade performance in image-text tasks, leading to inaccurate responses or hallucinated content. These errors are especially problematic in safety-critical domains like medical imaging or autonomous systems and reveal the brittleness of current fusion architectures ([SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025), [Ruh.ai](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026)).\n",
       "\n",
       "**Privacy and security** concerns are increasingly significant as multimodal LLMs process highly sensitive personal data that spans text, images, videos, and biometric signals. Ensuring data confidentiality during training and inference requires advanced techniques such as federated learning, differential privacy, and robust adversarial defenses. These safeguards remain immature, showing trade-offs between model utility and security guarantees. Furthermore, governance and ethical use around multimodal data remain active areas of policy discussion ([Hatchworks](https://hatchworks.com/blog/gen-ai/large-language-models-guide/), [Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\n",
       "\n",
       "Ongoing research efforts aim to address these limitations by improving both **efficiency and multimodal alignment**. Novel architectures leveraging hierarchical attention and cross-modal transformers seek better semantic fusion. Self-supervised multimodal pretraining enables models to learn richer joint representations from unlabeled data, enhancing contextual understanding. In addition, pruning, quantization, and hardware-aware optimizations are advancing resource-efficient model deployment—critical for edge and real-time applications ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models), [TekTarget](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\n",
       "\n",
       "Looking forward, future trends are expected to include tighter **integration of modalities** via unified model frameworks that dynamically weight sensor inputs based on context. Self-supervised multimodal learning will expand to more diverse and noisier data domains, improving robustness and adaptability. Finally, resource-optimized inference pipelines and modular composable architectures will enable scalable deployment without sacrificing multimodal semantic depth—a critical step for broader real-world adoption in 2026 and beyond ([Ruh.ai](https://www.ruh.ai/blogs/multimodal-ai-complete-guide-2026), [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\n",
       "\n",
       "```python\n",
       "# Example pseudocode for a modular multimodal fusion step combining text and image features:\n",
       "def multimodal_fusion(text_features, image_features):\n",
       "    # Dynamic weighting based on modality confidence scores\n",
       "    text_weight = compute_confidence(text_features)\n",
       "    image_weight = compute_confidence(image_features)\n",
       "\n",
       "    # Weighted sum fusion of embeddings\n",
       "    fused_embedding = text_weight * text_features + image_weight * image_features\n",
       "\n",
       "    # Pass through a context-aware transformer for reasoning\n",
       "    output = context_aware_transformer(fused_embedding)\n",
       "    return output\n",
       "```\n",
       "\n",
       "This highlights how future architectures might explicitly address modality imbalance and dynamic context adaptation, key foci for upcoming research.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(multimodal_llms['final']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8d467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-tutorial (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
