{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a7ac9b",
   "metadata": {},
   "source": [
    "## **Local MCP Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7244032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import HumanMessage\n",
    "from IPython.display import Markdown\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6214b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient({\n",
    "    \"Web Search\": {\n",
    "        \"command\": \"/home/muhammadumerkhan/.local/bin/uv\",\n",
    "        \"args\": [\n",
    "            \"run\",\n",
    "            \"--with\",\n",
    "            \"fastmcp\",\n",
    "            \"fastmcp\",\n",
    "            \"run\",\n",
    "            \"/home/muhammadumerkhan/Gen-Agentic-AI-Tutorials/LangChain Academy Course/Foundation/01_intro_to_langchain/module_2/2.1_mcp_server/2.1_mcp_server.py\"\n",
    "        ],\n",
    "        \"env\": {},\n",
    "        \"transport\": \"stdio\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04287455",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await client.get_tools()\n",
    "prompt = await client.get_prompt(\"Web Search\", \"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6932b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_web': StructuredTool(name='search_web', description='Search the web for information.', args_schema={'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}, metadata={'_meta': {'_fastmcp': {'tags': []}}}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x76ccb60ff7e0>)}\n",
      "====================================================================================================\n",
      "\n",
      "    You are a helpful assistant that answers user questions about LangChain, LangGraph and LangSmith.\n",
      "\n",
      "    You can use the following tools/resources to answer user questions:\n",
      "    - search_web: Search the web for information\n",
      "    - github_file: Access the langchain-ai repo files\n",
      "\n",
      "    If the user asks a question that is not related to LangChain, LangGraph or LangSmith, you should say \"I'm sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\"\n",
      "\n",
      "    You may try multiple tool and resource calls to answer the user's question.\n",
      "\n",
      "    You may also ask clarifying questions to the user to better understand their question.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print({tool.name: tool for tool in tools})\n",
    "print(\"==\"*50)\n",
    "print(prompt[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3da468",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt[0].content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428f39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \"configurable\": { \"thread_id\": \"1\" } }\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    { \"messages\" :\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=\"What is LangChain?\",\n",
    "                additional_kwargs=config\n",
    "            )\n",
    "        ]\n",
    "    }, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "341b2773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "'What is LangChain?'\n",
      "====================================================================================================\n",
      "Tool Used:\n",
      "[{'args': {'query': 'LangChain framework what is LangChain LangChain docs core '\n",
      "                    'concepts Chains Agents tools memory LangSmith LangGraph'},\n",
      "  'id': 'call_lq3loNHE9Nk4GchXddXgx5Dx',\n",
      "  'name': 'search_web',\n",
      "  'type': 'tool_call'}]\n",
      "====================================================================================================\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LangChain is an open-source framework designed for building applications that use large language models (LLMs). It provides high-level building blocks and abstractions that make it easier to connect LLMs to tools, data sources, memory, and orchestration logic, so you can build robust, real-world AI apps faster.\n",
       "\n",
       "Key ideas and components:\n",
       "- Chains: sequences of steps that process inputs and produce outputs, often combining LLM calls with other logic.\n",
       "- Tools: external functions or services (APIs, calculators, databases, search, etc.) that an LLM can call through an agent.\n",
       "- Agents: LLM-powered controllers that decide which tools or chains to invoke based on the current task and context.\n",
       "- Memory: mechanisms to remember past interactions or context across turns or sessions.\n",
       "- Prompts: templates and prompt management utilities to craft effective instructions for the LLM.\n",
       "- Providers and data sources: easy integration with major LLM providers (e.g., OpenAI, Anthropic, Google) and various data sources.\n",
       "\n",
       "Ecosystem and orchestration:\n",
       "- LangGraph: a low-level orchestration framework used under the hood by LangChain to manage complex, deterministic and agent-driven workflows.\n",
       "- LangSmith: observability and governance tooling to trace, debug, monitor, and measure AI runs and tool usage.\n",
       "- LangChain also includes or interplays with other components like LangFlow (UI tooling) and supports multi-language use (LangChain Python and LangChain.js/TypeScript).\n",
       "\n",
       "What it’s for:\n",
       "- Building practical AI apps such as chatbots, question-answering systems, data extraction/workflows, and automation that can decide to call tools or follow chains.\n",
       "- Enterprise-grade workflows where tool governance, monitoring, and orchestration are important.\n",
       "\n",
       "How to get started (high level):\n",
       "- LangChain is available in Python and JavaScript/TypeScript.\n",
       "- You can usually start with a few lines of code to connect to an LLM and begin composing chains, agents, and memories.\n",
       "- Official docs (overview, agents, LangGraph, LangSmith) are a great next step if you want concrete examples and code.\n",
       "\n",
       "If you’d like, I can pull up a quick starter example or point you to the official docs for Python or JavaScript. Do you want a short code sample or a link to the docs?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Question:\")\n",
    "pprint(response['messages'][0].content)\n",
    "\n",
    "print(\"==\"*50)\n",
    "\n",
    "print(\"Tool Used:\")\n",
    "pprint(response['messages'][1].tool_calls)\n",
    "\n",
    "print(\"==\"*50)\n",
    "\n",
    "print(\"Response:\")\n",
    "Markdown(response['messages'][3].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a6598",
   "metadata": {},
   "source": [
    "## **Remote MCP Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "716e0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60dcfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff15c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-tutorial (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
