{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd5ee5a",
   "metadata": {},
   "source": [
    "# Chatbot with Collection Schema \n",
    "\n",
    "## Review\n",
    "\n",
    "We extended our chatbot to save semantic memories to a single [user profile](https://docs.langchain.com/oss/python/concepts/memory#profile). \n",
    "\n",
    "We also introduced a library, [Trustcall](https://github.com/hinthornw/trustcall), to update this schema with new information. \n",
    "\n",
    "## Goals\n",
    "\n",
    "Sometimes we want to save memories to a [collection](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_200) rather than single profile. \n",
    "\n",
    "Here we'll update our chatbot to [save memories to a collection](https://docs.langchain.com/oss/python/concepts/memory#collection).\n",
    "\n",
    "We'll also show how to use Trustcall to update this collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "044b1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from trustcall import create_extractor\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import merge_message_runs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import display, Markdown, Image\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de0739",
   "metadata": {},
   "source": [
    "## Defining a collection schema\n",
    "\n",
    "Instead of storing user information in a fixed profile structure, we'll create a flexible collection schema to store memories about user interactions.\n",
    "\n",
    "Each memory will be stored as a separate entry with a single `content` field for the main information we want to remember\n",
    "\n",
    "This approach allows us to build an open-ended collection of memories that can grow and change as we learn more about the user.\n",
    "\n",
    "We can define a collection schema as a [Pydantic](https://docs.pydantic.dev/latest/) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8aa0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    )\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(\n",
    "        description=\"A list of memories about the user.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd0d84",
   "metadata": {},
   "source": [
    "We can used LangChain's chat model  [chat model](https://docs.langchain.com/oss/python/langchain/models) interface's [`with_structured_output`](https://docs.langchain.com/oss/python/langchain/models#structured-outputs) method to enforce structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388a94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9471155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind schema to model\n",
    "model_structured = model.with_structured_output(MemoryCollection)\n",
    "\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "memory_collection = model_structured.invoke([\n",
    "    HumanMessage(content=\"Hi, I'm Umer. I love hiking and wandering.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0937e05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Memory(content='User introduced themselves as Umer.'),\n",
       " Memory(content='User expressed a love for hiking.'),\n",
       " Memory(content='User enjoys wandering.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection.memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d93d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'User introduced themselves as Umer.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection.memories[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8759d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the in-memory store\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = str(1)\n",
    "memory_namespace = (user_id, \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "store.put(memory_namespace, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[1].model_dump()\n",
    "store.put(memory_namespace, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb94e9",
   "metadata": {},
   "source": [
    "Search for memories in the store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab46e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['1', 'memories'], key='b1c294b8-f2a8-4d6a-a422-91b4728d94d7', value={'content': 'User expressed a love for hiking.'}, created_at='2026-01-06T15:34:28.885688+00:00', updated_at='2026-01-06T15:34:28.885688+00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get(memory_namespace, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20082fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memories'], key='60da373f-0f7a-4590-854f-b8735bd4ad14', value={'content': 'User introduced themselves as Umer.'}, created_at='2026-01-06T15:34:28.885688+00:00', updated_at='2026-01-06T15:34:28.885688+00:00', score=None),\n",
       " Item(namespace=['1', 'memories'], key='b1c294b8-f2a8-4d6a-a422-91b4728d94d7', value={'content': 'User expressed a love for hiking.'}, created_at='2026-01-06T15:34:28.885688+00:00', updated_at='2026-01-06T15:34:28.885688+00:00', score=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.search(memory_namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8230dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['1', 'memories'], 'key': '60da373f-0f7a-4590-854f-b8735bd4ad14', 'value': {'content': 'User introduced themselves as Umer.'}, 'created_at': '2026-01-06T15:34:28.885688+00:00', 'updated_at': '2026-01-06T15:34:28.885688+00:00', 'score': None}\n",
      "{'namespace': ['1', 'memories'], 'key': 'b1c294b8-f2a8-4d6a-a422-91b4728d94d7', 'value': {'content': 'User expressed a love for hiking.'}, 'created_at': '2026-01-06T15:34:28.885688+00:00', 'updated_at': '2026-01-06T15:34:28.885688+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "for item in store.search(memory_namespace):\n",
    "    print(item.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cbb13",
   "metadata": {},
   "source": [
    "## Updating collection schema\n",
    "\n",
    "We discussed the challenges with updating a profile schema in the last lesson. \n",
    "\n",
    "The same applies for collections! \n",
    "\n",
    "We want the ability to update the collection with new memories as well as update existing memories in the collection. \n",
    "\n",
    "Now we'll show that [Trustcall](https://github.com/hinthornw/trustcall) can be also used to update a collection. \n",
    "\n",
    "This enables both addition of new memories as well as [updating existing memories in the collection](https://github.com/hinthornw/trustcall?tab=readme-ov-file#simultanous-updates--insertions\n",
    ").\n",
    "\n",
    "Let's define a new extractor with Trustcall. \n",
    "\n",
    "As before, we provide the schema for each memory, `Memory`.  \n",
    "\n",
    "But, we can supply `enable_inserts=True` to allow the extractor to insert new memories to the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2865b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a83617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Lance.\"), \n",
    "    AIMessage(content=\"Nice to meet you, Lance.\"), \n",
    "    HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")\n",
    "]\n",
    "\n",
    "# Run the extractor\n",
    "memory_collection = trustcall_extractor.invoke(\n",
    "    {\"messages\": [SystemMessage(content=instruction)] + conversation}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7030d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['messages', 'responses', 'response_metadata', 'attempts'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63499133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 108, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'id': 'chatcmpl-Cv3QBoxlnUyDYpliCoy8RigZk6VZe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b93f5-46f0-7b80-8da1-c8b4ea0cb31a-0', tool_calls=[{'name': 'Memory', 'args': {'content': 'Lance had a nice bike ride in San Francisco this morning.'}, 'id': 'call_kMXrQ7qCoN8bvNoySFrJdxIQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 108, 'output_tokens': 17, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c5d44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_kMXrQ7qCoN8bvNoySFrJdxIQ)\n",
      " Call ID: call_kMXrQ7qCoN8bvNoySFrJdxIQ\n",
      "  Args:\n",
      "    content: Lance had a nice bike ride in San Francisco this morning.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in memory_collection['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ecd0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_kMXrQ7qCoN8bvNoySFrJdxIQ'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in memory_collection[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c40b935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [\n",
    "    AIMessage(content=\"That's great, did you do after?\"), \n",
    "    HumanMessage(content=\"I went to Tartine and ate a croissant.\"),                        \n",
    "    AIMessage(content=\"What else is on your mind?\"),\n",
    "    HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\")\n",
    "]\n",
    "\n",
    "# update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "744292e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Memory(content='Lance had a nice bike ride in San Francisco this morning.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9059528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  Memory(content='Lance had a nice bike ride in San Francisco this morning.'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_mem = [(str(i), tool_name, m) for i, m in enumerate(memory_collection['responses'])] if memory_collection['responses'] else []\n",
    "existing_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f13ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "memory_collection = trustcall_extractor.invoke({\n",
    "    \"messages\": [SystemMessage(content=system_msg)] + updated_conversation, \n",
    "    \"existing\": existing_mem\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56bb3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_o4f8oSqVSIoc0W4sjLRYlXrp)\n",
      " Call ID: call_o4f8oSqVSIoc0W4sjLRYlXrp\n",
      "  Args:\n",
      "    content: Lance had a nice bike ride in San Francisco this morning.\n",
      "    -: Lance went to Tartine and ate a croissant after his bike ride in San Francisco.\n",
      "  Memory (call_mP92FQYT50Mopp8rQrJZ8eCK)\n",
      " Call ID: call_mP92FQYT50Mopp8rQrJZ8eCK\n",
      "  Args:\n",
      "    content: Lance is thinking about his trip to Japan and considering going back this winter.\n"
     ]
    }
   ],
   "source": [
    "# Messages from the model indicate two tool calls were made\n",
    "for m in memory_collection['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57eb89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lance had a nice bike ride in San Francisco this morning.\n",
      "==================================================\n",
      "Lance is thinking about his trip to Japan and considering going back this winter.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in memory_collection[\"responses\"]:\n",
    "    print(m.content)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6facbb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_o4f8oSqVSIoc0W4sjLRYlXrp', 'json_doc_id': '0'}\n",
      "{'id': 'call_mP92FQYT50Mopp8rQrJZ8eCK'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in memory_collection[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0a94ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Lance had a nice bike ride in San Francisco this morning.'}\n",
      "==================================================\n",
      "{'id': 'call_o4f8oSqVSIoc0W4sjLRYlXrp', 'json_doc_id': '0'}\n",
      "{'content': 'Lance is thinking about his trip to Japan and considering going back this winter.'}\n",
      "==================================================\n",
      "{'id': 'call_mP92FQYT50Mopp8rQrJZ8eCK'}\n"
     ]
    }
   ],
   "source": [
    "for r, m in zip(memory_collection['responses'], memory_collection['response_metadata']):\n",
    "    print(r.model_dump(mode=\"json\"))\n",
    "    print(\"=\"*50)\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88b4fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, m in zip(memory_collection['responses'], memory_collection['response_metadata']):\n",
    "    store.put(\n",
    "        memory_namespace, \n",
    "        m.get(\"json_doc_id\", str(uuid.uuid4())), \n",
    "        r.model_dump(mode=\"json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f9589bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memories'], key='60da373f-0f7a-4590-854f-b8735bd4ad14', value={'content': 'User introduced themselves as Umer.'}, created_at='2026-01-06T15:34:28.885688+00:00', updated_at='2026-01-06T15:34:28.885688+00:00', score=None),\n",
       " Item(namespace=['1', 'memories'], key='b1c294b8-f2a8-4d6a-a422-91b4728d94d7', value={'content': 'User expressed a love for hiking.'}, created_at='2026-01-06T15:34:28.885688+00:00', updated_at='2026-01-06T15:34:28.885688+00:00', score=None),\n",
       " Item(namespace=['1', 'memories'], key='0', value={'content': 'Lance had a nice bike ride in San Francisco this morning.'}, created_at='2026-01-06T15:58:20.767299+00:00', updated_at='2026-01-06T15:58:20.767299+00:00', score=None),\n",
       " Item(namespace=['1', 'memories'], key='1c6ff609-275b-4210-a231-b3807ccaef19', value={'content': 'Lance is thinking about his trip to Japan and considering going back this winter.'}, created_at='2026-01-06T15:58:20.767299+00:00', updated_at='2026-01-06T15:58:20.767299+00:00', score=None)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories = store.search(memory_namespace)\n",
    "memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "158948e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "User introduced themselves as Umer.\n",
       "User expressed a love for hiking.\n",
       "Lance had a nice bike ride in San Francisco this morning.\n",
       "Lance is thinking about his trip to Japan and considering going back this winter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\\n\".join(mem.value['content'] for mem in memories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7fa15bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key -> 60da373f-0f7a-4590-854f-b8735bd4ad14\n",
      "Value -> {'content': 'User introduced themselves as Umer.'}\n",
      "==================================================\n",
      "Key -> b1c294b8-f2a8-4d6a-a422-91b4728d94d7\n",
      "Value -> {'content': 'User expressed a love for hiking.'}\n",
      "==================================================\n",
      "Key -> 0\n",
      "Value -> {'content': 'Lance had a nice bike ride in San Francisco this morning.'}\n",
      "==================================================\n",
      "Key -> 1c6ff609-275b-4210-a231-b3807ccaef19\n",
      "Value -> {'content': 'Lance is thinking about his trip to Japan and considering going back this winter.'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for mem in memories:\n",
    "    print(\"Key ->\", mem.key)\n",
    "    print(\"Value ->\", mem.value)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a2e1206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('60da373f-0f7a-4590-854f-b8735bd4ad14',\n",
       "  'Memory',\n",
       "  {'content': 'User introduced themselves as Umer.'}),\n",
       " ('b1c294b8-f2a8-4d6a-a422-91b4728d94d7',\n",
       "  'Memory',\n",
       "  {'content': 'User expressed a love for hiking.'}),\n",
       " ('0',\n",
       "  'Memory',\n",
       "  {'content': 'Lance had a nice bike ride in San Francisco this morning.'}),\n",
       " ('1c6ff609-275b-4210-a231-b3807ccaef19',\n",
       "  'Memory',\n",
       "  {'content': 'Lance is thinking about his trip to Japan and considering going back this winter.'})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[   \n",
    "    (existing_itm.key, \"Memory\", existing_itm.value)\n",
    "    for existing_itm in memories\n",
    "] if memories else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce549784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='ABC', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi, I'm Lance.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_message_runs(\n",
    "    [SystemMessage(content=\"ABC\")] + [HumanMessage(content=\"Hi, I'm Lance.\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294badaa",
   "metadata": {},
   "source": [
    "## Chatbot with collection schema updating\n",
    "\n",
    "Now, let's bring Trustcall into our chatbot to create and update a memory collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e71fafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f824ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory schema\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e789857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True                 # This will allow the extractor to insert new memories to the collection.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1e25f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"\n",
    "    You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "    You have a long term memory which keeps track of information you learn about the user over time.\n",
    "    Current Memory (may include updated memories from this conversation): {memory}\n",
    "\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"\n",
    "    Reflect on following interaction. \n",
    "    Use the provided tools to retain any necessary memories about the user. \n",
    "    Use parallel tool calling to handle updates and insertions simultaneously:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d63c5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "\n",
    "    # system prompt\n",
    "    system_msg = [SystemMessage(content=MODEL_SYSTEM_MESSAGE.format(memory=info))]\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke(system_msg + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40cff9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # existing memories\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([   \n",
    "                            (existing_itm.key, tool_name, existing_itm.value)\n",
    "                            for existing_itm in memories\n",
    "        ] if memories else None)\n",
    "    \n",
    "    # system message\n",
    "    system_msg = [SystemMessage(content=TRUSTCALL_INSTRUCTION)]\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    updated_mssgs = list(merge_message_runs(\n",
    "        messages=system_msg + state[\"messages\"]\n",
    "    ))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    memory_collection = trustcall_extractor.invoke(\n",
    "        {\"messages\": updated_mssgs, \"existing\": existing_memories}\n",
    "    )\n",
    "\n",
    "    for r, m in zip(memory_collection['responses'], memory_collection['response_metadata']):\n",
    "        store.put(\n",
    "            memory_namespace, \n",
    "            m.get(\"json_doc_id\", str(uuid.uuid4())), \n",
    "            r.model_dump(mode=\"json\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2495fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c15d8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View\n",
    "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "380e2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Umer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Umer! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Umer\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01972ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I love to spend times alone.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds peaceful, Umer. Spending time alone can be a great way to relax and recharge. What do you enjoy doing during your alone time?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I love to spend times alone.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fe7926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace for the memory to save\n",
    "memory_namespace = (\"memories\", \"1\")\n",
    "\n",
    "# Retrieve memory from the store\n",
    "memories = across_thread_memory.search(memory_namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b468b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['memories', '1'], key='911d6871-9242-4fd0-b076-3deaec93f4ee', value={'content': \"User's name is Umer.\"}, created_at='2026-01-06T16:17:56.761914+00:00', updated_at='2026-01-06T16:17:56.761914+00:00', score=None),\n",
       " Item(namespace=['memories', '1'], key='42303e7d-d8cc-47a1-9d03-5810b9dd9d1e', value={'content': 'User loves to spend time alone.'}, created_at='2026-01-06T16:18:00.476685+00:00', updated_at='2026-01-06T16:18:00.476685+00:00', score=None)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "245c11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key -> 911d6871-9242-4fd0-b076-3deaec93f4ee\n",
      "Value -> {'content': \"User's name is Umer.\"}\n",
      "==================================================\n",
      "Key -> 42303e7d-d8cc-47a1-9d03-5810b9dd9d1e\n",
      "Value -> {'content': 'User loves to spend time alone.'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for mem in memories:\n",
    "    print(\"Key ->\", mem.key)\n",
    "    print(\"Value ->\", mem.value)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27fd2bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also love hiking.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hiking is a wonderful way to enjoy nature and get some exercise. Do you have any favorite trails or places you like to hike?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also love hiking.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9909a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you recommend for me to spend my time for myself?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you enjoy spending time alone and love hiking, you might consider exploring new hiking trails or perhaps even planning a solo hiking trip to a place you've never been before. If you're looking for something different, you could try activities like photography during your hikes, journaling about your experiences, or even learning about the local flora and fauna. These activities can enhance your hiking experience and provide a deeper connection with nature. How does that sound?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What do you recommend for me to spend my time for myself?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8362250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
