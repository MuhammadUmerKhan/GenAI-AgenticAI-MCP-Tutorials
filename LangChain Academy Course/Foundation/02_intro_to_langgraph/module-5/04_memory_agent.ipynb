{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Memory Agent\n",
    "\n",
    "## Review\n",
    "\n",
    "We created a chatbot that saves semantic memories to a single [user profile](https://docs.langchain.com/oss/python/concepts/memory#profile) or [collection](https://docs.langchain.com/oss/python/concepts/memory#collection).\n",
    "\n",
    "We introduced [Trustcall](https://github.com/hinthornw/trustcall) as a way to update either schema.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going to pull together the pieces we've learned to build an agent with long-term memory.\n",
    "\n",
    "Our agent, `task_mAIstro`, will help us manage a ToDo list! \n",
    "\n",
    "The chatbots we built previously *always* reflected on the conversation and saved memories. \n",
    "\n",
    "`task_mAIstro` will decide *when* to save memories (items to our ToDo list).\n",
    "\n",
    "The chatbots we built previously always saved one type of memory, a profile or a collection. \n",
    "\n",
    "`task_mAIstro` can decide to save to either a user profile or a collection of ToDo items.\n",
    "\n",
    "In addition semantic memory, `task_mAIstro` also will manage procedural memory.\n",
    "\n",
    "This allows the user to update their preferences for creating ToDo items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pprint import pprint\n",
    "from helper import *\n",
    "from pydantic import BaseModel, Field\n",
    "from trustcall import create_extractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, merge_message_runs\n",
    "from typing import TypedDict, Literal, Optional, List\n",
    "from IPython.display import Image, display\n",
    "from datetime import datetime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, END, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Check if the variable is set in the OS environment\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # If not set, prompt the user for input\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "    \n",
    "    # Set the environment variable for the current process\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility into Trustcall updates\n",
    "\n",
    "Trustcall creates and updates JSON schemas.\n",
    "\n",
    "What if we want visibility into the *specific changes* made by Trustcall?\n",
    "\n",
    "For example, we saw before that Trustcall has some of its own tools to:\n",
    "\n",
    "* Self-correct from validation failures -- [see trace example here](https://smith.langchain.com/public/5cd23009-3e05-4b00-99f0-c66ee3edd06e/r/9684db76-2003-443b-9aa2-9a9dbc5498b7) \n",
    "* Update existing documents -- [see trace example here](https://smith.langchain.com/public/f45bdaf0-6963-4c19-8ec9-f4b7fe0f68ad/r/760f90e1-a5dc-48f1-8c34-79d6a3414ac3)\n",
    "\n",
    "Visibility into these tools can be useful for the agent we're going to build.\n",
    "\n",
    "Below, we'll show how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    )\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: List[Memory] = Field(\n",
    "        description=\"A list of memories about the user.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can add a [listener](https://python.langchain.com/docs/how_to/lcel_cheatsheet/#add-lifecycle-listeners) <!-- broken, but cannot find better linke --> to the Trustcall extractor.\n",
    "\n",
    "This will pass runs from the extractor's execution to a class, `Spy`, that we will define.\n",
    "\n",
    "Our `Spy` class will extract information about what tool calls were made by Trustcall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the tool calls made by Trustcall\n",
    "class Spy:\n",
    "    def __init__(self):\n",
    "        self.called_tools = []\n",
    "\n",
    "    def __call__(self, run):\n",
    "        # Collect information about the tool calls made by the extractor.\n",
    "        q = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the spy\n",
    "spy = Spy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "model.invoke(\"Hello\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm=model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True\n",
    ")\n",
    "\n",
    "# Add the spy as a listener\n",
    "trustcall_extractor_see_all_tool_calls = trustcall_extractor.with_listeners(on_end=spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Lance.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Lance.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_fmj4zUGzzloAl53UWyoO2Nbp)\n",
      " Call ID: call_fmj4zUGzzloAl53UWyoO2Nbp\n",
      "  Args:\n",
      "    content: User had a nice bike ride in San Francisco this morning.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User had a nice bike ride in San Francisco this morning.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_fmj4zUGzzloAl53UWyoO2Nbp'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  {'content': 'User had a nice bike ride in San Francisco this morning.'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [AIMessage(content=\"That's great, did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Tartine and ate a croissant.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\"),]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "existing_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trustcall_extractor_see_all_tool_calls.invoke({\"messages\": updated_conversation, \n",
    "                                                        \"existing\": existing_memories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_RSK5Jkxw7iF8D6EhzrEVOvpy', 'json_doc_id': '0'}\n",
      "{'id': 'call_a2gle81QZajEnOdErlgMLSK6'}\n",
      "{'id': 'call_GIgsgRMJOkVdlRxiPgSMA6fP'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_RSK5Jkxw7iF8D6EhzrEVOvpy)\n",
      " Call ID: call_RSK5Jkxw7iF8D6EhzrEVOvpy\n",
      "  Args:\n",
      "    content: User went to Tartine and ate a croissant.\n",
      "    -: {'content': 'User is thinking about going back to Japan this winter.'}\n",
      "  Memory (call_a2gle81QZajEnOdErlgMLSK6)\n",
      " Call ID: call_a2gle81QZajEnOdErlgMLSK6\n",
      "  Args:\n",
      "    content: User went to Tartine and ate a croissant.\n",
      "  Memory (call_GIgsgRMJOkVdlRxiPgSMA6fP)\n",
      " Call ID: call_GIgsgRMJOkVdlRxiPgSMA6fP\n",
      "  Args:\n",
      "    content: User is thinking about going back to Japan this winter.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User went to Tartine and ate a croissant.'\n",
      "content='User went to Tartine and ate a croissant.'\n",
      "content='User is thinking about going back to Japan this winter.'\n"
     ]
    }
   ],
   "source": [
    "# Parsed responses\n",
    "for m in result[\"responses\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'name': 'PatchDoc',\n",
       "   'args': {'json_doc_id': '0',\n",
       "    'planned_edits': '1. Replace the existing content with a new memory about visiting Tartine and eating a croissant. 2. Add a new memory about planning to go back to Japan this winter.',\n",
       "    'patches': [{'op': 'replace',\n",
       "      'path': '/content',\n",
       "      'value': 'User went to Tartine and ate a croissant.'},\n",
       "     {'op': 'add',\n",
       "      'path': '/-',\n",
       "      'value': {'content': 'User is thinking about going back to Japan this winter.'}}]},\n",
       "   'id': 'call_RSK5Jkxw7iF8D6EhzrEVOvpy',\n",
       "   'type': 'tool_call'},\n",
       "  {'name': 'Memory',\n",
       "   'args': {'content': 'User went to Tartine and ate a croissant.'},\n",
       "   'id': 'call_a2gle81QZajEnOdErlgMLSK6',\n",
       "   'type': 'tool_call'},\n",
       "  {'name': 'Memory',\n",
       "   'args': {'content': 'User is thinking about going back to Japan this winter.'},\n",
       "   'id': 'call_GIgsgRMJOkVdlRxiPgSMA6fP',\n",
       "   'type': 'tool_call'}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy.called_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_info(tool_calls, schema_name=\"Memory\"):\n",
    "    \"\"\"Extract information from tool calls for both patches and new memories.\n",
    "    \n",
    "    Args:\n",
    "        tool_calls: List of tool calls from the model\n",
    "        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize list of changes\n",
    "    changes = []\n",
    "    \n",
    "    for call_group in tool_calls:\n",
    "        for call in call_group:\n",
    "            if call['name'] == 'PatchDoc':\n",
    "                changes.append({\n",
    "                    'type': 'update',\n",
    "                    'doc_id': call['args']['json_doc_id'],\n",
    "                    'planned_edits': call['args']['planned_edits'],\n",
    "                    'value': call['args']['patches'][0]['value']\n",
    "                })\n",
    "            elif call['name'] == schema_name:\n",
    "                changes.append({\n",
    "                    'type': 'new',\n",
    "                    'value': call['args']\n",
    "                })\n",
    "\n",
    "    # Format results as a single string\n",
    "    result_parts = []\n",
    "    for change in changes:\n",
    "        if change['type'] == 'update':\n",
    "            result_parts.append(\n",
    "                f\"Document {change['doc_id']} updated:\\n\"\n",
    "                f\"Plan: {change['planned_edits']}\\n\"\n",
    "                f\"Added content: {change['value']}\"\n",
    "            )\n",
    "        else:\n",
    "            result_parts.append(\n",
    "                f\"New {schema_name} created:\\n\"\n",
    "                f\"Content: {change['value']}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(result_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 updated:\n",
      "Plan: 1. Replace the existing content with a new memory about visiting Tartine and eating a croissant. 2. Add a new memory about planning to go back to Japan this winter.\n",
      "Added content: User went to Tartine and ate a croissant.\n",
      "\n",
      "New Memory created:\n",
      "Content: {'content': 'User went to Tartine and ate a croissant.'}\n",
      "\n",
      "New Memory created:\n",
      "Content: {'content': 'User is thinking about going back to Japan this winter.'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect spy.called_tools to see exactly what happened during the extraction\n",
    "schema_name = \"Memory\"\n",
    "changes = extract_tool_info(spy.called_tools, schema_name)\n",
    "print(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Creating an agent\n",
    "\n",
    "There are many different agent architectures to choose from.\n",
    "\n",
    "Here, we'll implement something simple, a [ReAct](https://docs.langchain.com/oss/python/langgraph/workflows-agents#agents) agent.\n",
    "\n",
    "This agent will be a helpful companion for creating and managing a ToDo list.\n",
    "\n",
    "This agent can make a decision to update three types of long-term memory: \n",
    "\n",
    "(a) Create or update a user `profile` with general user information \n",
    "\n",
    "(b) Add or update items in a ToDo list `collection`\n",
    "\n",
    "(c) Update its own `instructions` on how to update items to the ToDo list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateMemory(TypedDict):\n",
    "    \"\"\" Decision on what memory type to update \"\"\"\n",
    "    update_type: Literal['user', 'todo', 'instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User profile schema\n",
    "class Profile(BaseModel):\n",
    "    \"\"\"This is the profile of the user you are chatting with\"\"\"\n",
    "    \n",
    "    name: Optional[str] = Field(\n",
    "        description=\"The name of the user\", default=None\n",
    "    )\n",
    "    location: Optional[str] = Field(\n",
    "        description=\"The user's location\", default=None\n",
    "    )\n",
    "    job: Optional[str] = Field(\n",
    "        description=\"The user's job\", default=None\n",
    "    )\n",
    "    connections: list[str] = Field(\n",
    "        description=\"Personal connection of the user, such as family members, friends, or coworkers\", default=list\n",
    "    )\n",
    "    interests: list[str] = Field(\n",
    "        description=\"Interests that the user has\", default_factory=list\n",
    "    )\n",
    "\n",
    "# ToDo schema\n",
    "class ToDo(BaseModel):\n",
    "    \"\"\"This is the to do list of the user\"\"\"\n",
    "    \n",
    "    tasks: List[str] = Field(\n",
    "        description=\"The tasks that the user has to do\", default=list\n",
    "    )\n",
    "    time_to_complete: Optional[int] = Field(\n",
    "        description=\"Estimated time to complete the tasks (minutes)\", default=None\n",
    "    )\n",
    "    deadline: Optional[datetime] = Field(\n",
    "        description=\"When the task needs to be completed by (if applicable)\", default=None\n",
    "    )\n",
    "    solutions: list[str] = Field(\n",
    "        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n",
    "        min_length=1,\n",
    "        default_factory=list\n",
    "    )\n",
    "    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n",
    "        description=\"Current status of the task\",\n",
    "        default=\"not started\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph definition \n",
    "\n",
    "We add a simple router, `route_message`, that makes a binary decision to save memories.\n",
    "\n",
    "The memory collection updating is handled by `Trustcall` in the `write_memory` node, as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trustcall extractor for updating the user profile \n",
    "profile_extractor = create_extractor(\n",
    "    llm=model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_mAIstro(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve task memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "    \n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile=user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_profile(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"profile\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Profile\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    TRUSTCALL_INSTRUCTION_FORMATTED=TRUSTCALL_INSTRUCTION.format(time=datetime.now().isoformat())\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + state[\"messages\"][:-1]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = profile_extractor.invoke({\"messages\": updated_messages, \n",
    "                                         \"existing\": existing_memories})\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    return {\"messages\": [{\"role\": \"tool\", \"content\": \"updated profile\", \"tool_call_id\":tool_calls[0]['id']}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_todos(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"todo\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"ToDo\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    TRUSTCALL_INSTRUCTION_FORMATTED=TRUSTCALL_INSTRUCTION.format(time=datetime.now().isoformat())\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + state[\"messages\"][:-1]))\n",
    "\n",
    "    # Initialize the spy for visibility into the tool calls made by Trustcall\n",
    "    spy = Spy()\n",
    "    \n",
    "    # Create the Trustcall extractor for updating the ToDo list \n",
    "    todo_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[ToDo],\n",
    "    tool_choice=tool_name,\n",
    "    enable_inserts=True\n",
    "    ).with_listeners(on_end=spy)\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = todo_extractor.invoke({\"messages\": updated_messages, \n",
    "                                    \"existing\": existing_memories})\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "        \n",
    "    # Respond to the tool call made in task_mAIstro, confirming the update\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    # Extract the changes made by Trustcall and add the the ToolMessage returned to task_mAIstro\n",
    "    todo_update_msg = extract_tool_info(spy.called_tools, tool_name)\n",
    "    return {\"messages\": [{\"role\": \"tool\", \"content\": todo_update_msg, \"tool_call_id\":tool_calls[0]['id']}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_instructions(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    \n",
    "    namespace = (\"instructions\", user_id)\n",
    "\n",
    "    existing_memory = store.get(namespace, \"user_instructions\")\n",
    "        \n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = CREATE_INSTRUCTIONS.format(current_instructions=existing_memory.value if existing_memory else None)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'][:-1] + [HumanMessage(content=\"Please update the instructions based on the conversation\")])\n",
    "\n",
    "    # Overwrite the existing memory in the store \n",
    "    key = \"user_instructions\"\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    return {\"messages\": [{\"role\": \"tool\", \"content\": \"updated instructions\", \"tool_call_id\":tool_calls[0]['id']}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional edge\n",
    "def route_message(state: MessagesState, config: RunnableConfig, store: BaseStore) -> Literal[END, \"update_todos\", \"update_instructions\", \"update_profile\"]:\n",
    "\n",
    "    \"\"\"Reflect on the memories and chat history to decide whether to update the memory collection.\"\"\"\n",
    "    message = state['messages'][-1]\n",
    "    if len(message.tool_calls) ==0:\n",
    "        return END\n",
    "    else:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        if tool_call['args']['update_type'] == \"user\":\n",
    "            return \"update_profile\"\n",
    "        elif tool_call['args']['update_type'] == \"todo\":\n",
    "            return \"update_todos\"\n",
    "        elif tool_call['args']['update_type'] == \"instructions\":\n",
    "            return \"update_instructions\"\n",
    "        else:\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# add graph nodes\n",
    "builder.add_node(\"mAIstro\", task_mAIstro)\n",
    "builder.add_node(\"update_todos\", update_todos)\n",
    "builder.add_node(\"update_profile\", update_profile)\n",
    "builder.add_node(\"update_instructions\", update_instructions)\n",
    "\n",
    "# add edges\n",
    "builder.add_edge(START, \"mAIstro\")\n",
    "builder.add_conditional_edges(\"mAIstro\", route_message)\n",
    "builder.add_edge(\"update_todos\", \"mAIstro\")\n",
    "builder.add_edge(\"update_profile\", \"mAIstro\")\n",
    "builder.add_edge(\"update_instructions\", \"mAIstro\")\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# We compile the graph with the checkpointer and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is Umer. I live in Karachi with my family. I am single.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_WfMPCqvYMPXU6EDTXiyA5nmA)\n",
      " Call ID: call_WfMPCqvYMPXU6EDTXiyA5nmA\n",
      "  Args:\n",
      "    update_type: user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Gen-Agentic-AI-Tutorials\\.venv\\Lib\\site-packages\\pydantic\\json_schema.py:2448: PydanticJsonSchemaWarning: Default value <class 'list'> is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "updated profile\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've updated your profile with your information. If there's anything else you'd like to share or update, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"Umer\"}}\n",
    "\n",
    "# User input to create a profile memory\n",
    "input_messages = [HumanMessage(content=\"My name is Umer. I live in Karachi with my family. I am single.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My wife asked me to book swim lessons for the baby.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_Tzcd0xjcxKwoMrx4zbAoD4yJ)\n",
      " Call ID: call_Tzcd0xjcxKwoMrx4zbAoD4yJ\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'tasks': ['Book swim lessons for the baby'], 'status': 'not started'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the task to book swim lessons for the baby to your ToDo list. If you have any more tasks or updates, feel free to share!\n"
     ]
    }
   ],
   "source": [
    "# User input for a ToDo\n",
    "input_messages = [HumanMessage(content=\"My wife asked me to book swim lessons for the baby.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "When creating or updating ToDo items, include specific local businesses / vendors.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_GbdjRBK4oFqQtXfWCl9YTZSA)\n",
      " Call ID: call_GbdjRBK4oFqQtXfWCl9YTZSA\n",
      "  Args:\n",
      "    update_type: instructions\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "updated instructions\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've noted your preference to include specific local businesses or vendors when creating or updating ToDo items. If you have any other preferences or tasks, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# User input to update instructions for creating ToDos\n",
    "input_messages = [HumanMessage(content=\"When creating or updating ToDo items, include specific local businesses / vendors.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': 'Based on your feedback, here are the updated instructions for how to update ToDo list items:\\n\\n1. When creating or updating ToDo items, always include specific local businesses or vendors relevant to the task, if applicable.\\n2. Ensure that tasks are clear and actionable, providing any necessary details that may help in completing the task.\\n\\nIf you have any more preferences or changes, just let me know!'}\n"
     ]
    }
   ],
   "source": [
    "# Check for updated instructions\n",
    "user_id = \"Umer\"\n",
    "\n",
    "# Search \n",
    "for memory in across_thread_memory.search((\"instructions\", user_id)):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I need to fix the jammed electric Yale lock on the door.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_IDgQ0wKKDESzWgKto9Fsktsq)\n",
      " Call ID: call_IDgQ0wKKDESzWgKto9Fsktsq\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'tasks': ['Fix the jammed electric Yale lock on the door'], 'status': 'not started', 'solutions': ['Contact a local locksmith', 'Check online for DIY repair guides']}\n",
      "\n",
      "Document 6d2afb8d-404e-401a-a9b5-6fc51cfdacf5 updated:\n",
      "Plan: Add a new task to fix the jammed electric Yale lock on the door.\n",
      "Added content: Fix the jammed electric Yale lock on the door\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the task to fix the jammed electric Yale lock on the door to your ToDo list. If you need any assistance with this task or have more to add, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# User input for a ToDo\n",
    "input_messages = [HumanMessage(content=\"I need to fix the jammed electric Yale lock on the door.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': ['Book swim lessons for the baby', 'Fix the jammed electric Yale lock on the door'], 'time_to_complete': None, 'deadline': None, 'solutions': ['Contact a local locksmith', 'Check online for DIY repair guides'], 'status': 'not started'}\n",
      "{'tasks': ['Fix the jammed electric Yale lock on the door'], 'time_to_complete': None, 'deadline': None, 'solutions': ['Contact a local locksmith', 'Check online for DIY repair guides'], 'status': 'not started'}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"Umer\"\n",
    "\n",
    "# Search \n",
    "for memory in across_thread_memory.search((\"todo\", user_id)):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "For the swim lessons, I need to get that done by end of November.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_jYwN8suDWRAD6IR4rOsuXilT)\n",
      " Call ID: call_jYwN8suDWRAD6IR4rOsuXilT\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Document 6d2afb8d-404e-401a-a9b5-6fc51cfdacf5 updated:\n",
      "Plan: Update the deadline for swim lessons to end of November.\n",
      "Added content: 2025-11-30T23:59:59\n",
      "\n",
      "Document cec4f48b-8b05-45a3-844c-f3a639e250c4 updated:\n",
      "Plan: Update the deadline for swim lessons to end of November.\n",
      "Added content: 2025-11-30T23:59:59\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've updated the deadline for the swim lessons to the end of November. If there's anything else you need to add or modify, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# User input to update an existing ToDo\n",
    "input_messages = [HumanMessage(content=\"For the swim lessons, I need to get that done by end of November.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Need to call back City Toyota to schedule car service.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_AumpS2YSeyunaWRV9FvktR2S)\n",
      " Call ID: call_AumpS2YSeyunaWRV9FvktR2S\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'tasks': ['Call back City Toyota to schedule car service'], 'status': 'not started'}\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'tasks': ['Call back City Toyota to schedule car service'], 'status': 'not started'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the task to call back City Toyota to schedule car service to your ToDo list. If you have any more tasks or updates, feel free to share!\n"
     ]
    }
   ],
   "source": [
    "# User input for a ToDo\n",
    "input_messages = [HumanMessage(content=\"Need to call back City Toyota to schedule car service.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': ['Book swim lessons for the baby', 'Fix the jammed electric Yale lock on the door'], 'time_to_complete': None, 'deadline': '2025-11-30T23:59:59', 'solutions': ['Contact a local locksmith', 'Check online for DIY repair guides'], 'status': 'not started'}\n",
      "{'tasks': ['Fix the jammed electric Yale lock on the door'], 'time_to_complete': None, 'deadline': '2025-11-30T23:59:59', 'solutions': ['Contact a local locksmith', 'Check online for DIY repair guides'], 'status': 'not started'}\n",
      "{'tasks': ['Call back City Toyota to schedule car service'], 'time_to_complete': None, 'deadline': None, 'solutions': [], 'status': 'not started'}\n",
      "{'tasks': ['Call back City Toyota to schedule car service'], 'time_to_complete': None, 'deadline': None, 'solutions': [], 'status': 'not started'}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"Umer\"\n",
    "\n",
    "# Search \n",
    "for memory in across_thread_memory.search((\"todo\", user_id)):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I have 30 minutes, what tasks can I get done?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You have a couple of tasks on your ToDo list that you might be able to tackle in 30 minutes:\n",
      "\n",
      "1. **Call back City Toyota to schedule car service** - This task doesn't have a specific time requirement, so it could likely be completed within your available time.\n",
      "\n",
      "Would you like to focus on this task, or do you want to prioritize something else?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"Umer\"}}\n",
    "\n",
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"I have 30 minutes, what tasks can I get done?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Yes, give me some options to call for swim lessons.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are a few options for swim lessons in Karachi that you can consider:\n",
      "\n",
      "1. **Swim Karachi** - They offer various swimming classes for children and adults. You can check their website or call them for more details.\n",
      "\n",
      "2. **Aqua Fitness** - Known for their structured swim programs for kids. They might have flexible timings that suit your schedule.\n",
      "\n",
      "3. **The Karachi Swim School** - They provide lessons for all ages and skill levels. Itâ€™s worth reaching out to see what they offer for babies.\n",
      "\n",
      "4. **Local Community Pools** - Many community pools offer swim lessons for children. You can check with your local community center for options.\n",
      "\n",
      "Would you like to proceed with contacting one of these options, or do you need more information?\n"
     ]
    }
   ],
   "source": [
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"Yes, give me some options to call for swim lessons.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Trustcall performs patching of the existing memory:\n",
    "\n",
    "https://smith.langchain.com/public/4ad3a8af-3b1e-493d-b163-3111aa3d575a/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new thread.\n",
    "\n",
    "This creates a new session. \n",
    "\n",
    "Profile, ToDos, and Instructions saved to long-term memory are accessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/84768705-be91-43e4-8a6f-f9d3cee93782/r\n",
    "\n",
    "## Studio\n",
    "\n",
    "![Screenshot 2024-11-04 at 1.00.19 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6732cfb05d9709862eba4e6c_Screenshot%202024-11-11%20at%207.46.40%E2%80%AFPM.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
