{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8517d450",
   "metadata": {},
   "source": [
    "# Memory Agent\n",
    "\n",
    "## Review\n",
    "\n",
    "We created a chatbot that saves semantic memories to a single [user profile](https://docs.langchain.com/oss/python/concepts/memory#profile) or [collection](https://docs.langchain.com/oss/python/concepts/memory#collection).\n",
    "\n",
    "We introduced [Trustcall](https://github.com/hinthornw/trustcall) as a way to update either schema.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going to pull together the pieces we've learned to build an agent with long-term memory.\n",
    "\n",
    "Our agent, `task_mAIstro`, will help us manage a ToDo list! \n",
    "\n",
    "The chatbots we built previously *always* reflected on the conversation and saved memories. \n",
    "\n",
    "`task_mAIstro` will decide *when* to save memories (items to our ToDo list).\n",
    "\n",
    "The chatbots we built previously always saved one type of memory, a profile or a collection. \n",
    "\n",
    "`task_mAIstro` can decide to save to either a user profile or a collection of ToDo items.\n",
    "\n",
    "In addition semantic memory, `task_mAIstro` also will manage procedural memory.\n",
    "\n",
    "This allows the user to update their preferences for creating ToDo items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d147f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pprint import pprint\n",
    "from helper import *\n",
    "from pydantic import BaseModel, Field\n",
    "from trustcall import create_extractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, merge_message_runs\n",
    "from typing import TypedDict, Literal, Optional, List\n",
    "from IPython.display import Image, display\n",
    "from datetime import datetime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, END, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a9daf",
   "metadata": {},
   "source": [
    "## Visibility into Trustcall updates\n",
    "\n",
    "Trustcall creates and updates JSON schemas.\n",
    "\n",
    "What if we want visibility into the *specific changes* made by Trustcall?\n",
    "\n",
    "For example, we saw before that Trustcall has some of its own tools to:\n",
    "\n",
    "* Self-correct from validation failures -- [see trace example here](https://smith.langchain.com/public/5cd23009-3e05-4b00-99f0-c66ee3edd06e/r/9684db76-2003-443b-9aa2-9a9dbc5498b7) \n",
    "* Update existing documents -- [see trace example here](https://smith.langchain.com/public/f45bdaf0-6963-4c19-8ec9-f4b7fe0f68ad/r/760f90e1-a5dc-48f1-8c34-79d6a3414ac3)\n",
    "\n",
    "Visibility into these tools can be useful for the agent we're going to build.\n",
    "\n",
    "Below, we'll show how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6e4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    )\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: List[Memory] = Field(\n",
    "        description=\"A list of memories about the user.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ae25f",
   "metadata": {},
   "source": [
    "When storing objects (e.g., memories) in the [Store](https://reference.langchain.com/python/langgraph/store/?h=basestor#langgraph.store.base.BaseStore), we provide:\n",
    "\n",
    "- The `namespace` for the object, a tuple (similar to directories)\n",
    "- the object `key` (similar to filenames)\n",
    "- the object `value` (similar to file contents)\n",
    "\n",
    "We use the [put](https://reference.langchain.com/python/langgraph/store/?h=basestor#langgraph.store.base.BaseStore.put) method to save an object to the store by `namespace` and `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9475b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the tool calls made by Trustcall\n",
    "class Spy:\n",
    "    def __init__(self):\n",
    "        self.called_tools = []\n",
    "\n",
    "    def __call__(self, run):\n",
    "        # Collect information about the tool calls made by the extractor.\n",
    "        q = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483a48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = Spy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab6195a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "model.invoke(\"Hello\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c437d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_call_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True\n",
    ")\n",
    "\n",
    "# Add the spy as a listener\n",
    "trust_call_extractor_with_listner = trust_call_extractor.with_listeners(on_end=spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162aa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Umer.\"), \n",
    "    AIMessage(content=\"Nice to meet you, Umer.\"), \n",
    "    HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")\n",
    "]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trust_call_extractor.invoke({\n",
    "    \"messages\": [SystemMessage(content=instruction)] + conversation\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7794fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_cnpn4mv0MbmqDUfdg1l1Km0z)\n",
      " Call ID: call_cnpn4mv0MbmqDUfdg1l1Km0z\n",
      "  Args:\n",
      "    content: User had a nice bike ride in San Francisco this morning.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246c0f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User had a nice bike ride in San Francisco this morning.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba255ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_cnpn4mv0MbmqDUfdg1l1Km0z'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9142df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [\n",
    "    AIMessage(content=\"That's great, did you do after?\"), \n",
    "    HumanMessage(content=\"I went to Tartine and ate a croissant.\"),                        \n",
    "    AIMessage(content=\"What else is on your mind?\"),\n",
    "    HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\")\n",
    "]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063c031c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  Memory(content='User had a nice bike ride in San Francisco this morning.'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_mem = [(str(i), tool_name, m) for i, m in enumerate(result['responses'])] if result['responses'] else []\n",
    "existing_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e087e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trust_call_extractor_with_listner.invoke({\n",
    "    \"messages\": updated_conversation,\n",
    "    \"existing\": existing_mem\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bddcd73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['messages', 'responses', 'response_metadata', 'attempts'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1311d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_OWEe21yDLQbWkNTtD5zahECY', 'json_doc_id': '0'}\n",
      "{'id': 'call_VXHdQtBLOu2jxkkPeQIgXb6f'}\n",
      "{'id': 'call_TbquOFUIluO7KzSMrjqYXXe6'}\n"
     ]
    }
   ],
   "source": [
    "for m in result['response_metadata']:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "137a368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_OWEe21yDLQbWkNTtD5zahECY)\n",
      " Call ID: call_OWEe21yDLQbWkNTtD5zahECY\n",
      "  Args:\n",
      "    content: User went to Tartine and ate a croissant.\n",
      "    -: {'content': 'User is thinking about going back to Japan this winter.'}\n",
      "  Memory (call_VXHdQtBLOu2jxkkPeQIgXb6f)\n",
      " Call ID: call_VXHdQtBLOu2jxkkPeQIgXb6f\n",
      "  Args:\n",
      "    content: User went to Tartine and ate a croissant.\n",
      "  Memory (call_TbquOFUIluO7KzSMrjqYXXe6)\n",
      " Call ID: call_TbquOFUIluO7KzSMrjqYXXe6\n",
      "  Args:\n",
      "    content: User is thinking about going back to Japan this winter.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1e9f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User went to Tartine and ate a croissant.'\n",
      "content='User went to Tartine and ate a croissant.'\n",
      "content='User is thinking about going back to Japan this winter.'\n"
     ]
    }
   ],
   "source": [
    "# Parsed responses\n",
    "for m in result[\"responses\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f48e26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'name': 'PatchDoc',\n",
       "   'args': {'json_doc_id': '0',\n",
       "    'planned_edits': '1. Replace the existing content with a new memory about visiting Tartine and eating a croissant. 2. Add a new memory about planning a trip to Japan this winter.',\n",
       "    'patches': [{'op': 'replace',\n",
       "      'path': '/content',\n",
       "      'value': 'User went to Tartine and ate a croissant.'},\n",
       "     {'op': 'add',\n",
       "      'path': '/-',\n",
       "      'value': {'content': 'User is thinking about going back to Japan this winter.'}}]},\n",
       "   'id': 'call_OWEe21yDLQbWkNTtD5zahECY',\n",
       "   'type': 'tool_call'},\n",
       "  {'name': 'Memory',\n",
       "   'args': {'content': 'User went to Tartine and ate a croissant.'},\n",
       "   'id': 'call_VXHdQtBLOu2jxkkPeQIgXb6f',\n",
       "   'type': 'tool_call'},\n",
       "  {'name': 'Memory',\n",
       "   'args': {'content': 'User is thinking about going back to Japan this winter.'},\n",
       "   'id': 'call_TbquOFUIluO7KzSMrjqYXXe6',\n",
       "   'type': 'tool_call'}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the tool calls made by Trustcall\n",
    "spy.called_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01d8a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_info(tool_calls, schema_name=\"Memory\"):\n",
    "    \"\"\"Extract information from tool calls for both patches and new memories.\n",
    "    \n",
    "    Args:\n",
    "        tool_calls: List of tool calls from the model\n",
    "        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize list of changes\n",
    "    changes = []\n",
    "    \n",
    "    for call_group in tool_calls:\n",
    "        for call in call_group:\n",
    "            if call['name'] == 'PatchDoc':\n",
    "                changes.append({\n",
    "                    'type': 'update',\n",
    "                    'doc_id': call['args']['json_doc_id'],\n",
    "                    'planned_edits': call['args']['planned_edits'],\n",
    "                    'value': call['args']['patches'][0]['value']\n",
    "                })\n",
    "            elif call['name'] == schema_name:\n",
    "                changes.append({\n",
    "                    'type': 'new',\n",
    "                    'value': call['args']\n",
    "                })\n",
    "\n",
    "    # Format results as a single string\n",
    "    result_parts = []\n",
    "    for change in changes:\n",
    "        if change['type'] == 'update':\n",
    "            result_parts.append(\n",
    "                f\"Document {change['doc_id']} updated:\\n\"\n",
    "                f\"Plan: {change['planned_edits']}\\n\"\n",
    "                f\"Added content: {change['value']}\"\n",
    "            )\n",
    "        else:\n",
    "            result_parts.append(\n",
    "                f\"New {schema_name} created:\\n\"\n",
    "                f\"Content: {change['value']}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(result_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42cfe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 updated:\n",
      "Plan: 1. Replace the existing content with a new memory about visiting Tartine and eating a croissant. 2. Add a new memory about planning a trip to Japan this winter.\n",
      "Added content: User went to Tartine and ate a croissant.\n",
      "\n",
      "New Memory created:\n",
      "Content: {'content': 'User went to Tartine and ate a croissant.'}\n",
      "\n",
      "New Memory created:\n",
      "Content: {'content': 'User is thinking about going back to Japan this winter.'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect spy.called_tools to see exactly what happened during the extraction\n",
    "schema_name = \"Memory\"\n",
    "changes = extract_tool_info(spy.called_tools, schema_name)\n",
    "print(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc59e2b",
   "metadata": {},
   "source": [
    "## Creating an agent\n",
    "\n",
    "There are many different agent architectures to choose from.\n",
    "\n",
    "Here, we'll implement something simple, a [ReAct](https://docs.langchain.com/oss/python/langgraph/workflows-agents#agents) agent.\n",
    "\n",
    "This agent will be a helpful companion for creating and managing a ToDo list.\n",
    "\n",
    "This agent can make a decision to update three types of long-term memory: \n",
    "\n",
    "(a) Create or update a user `profile` with general user information \n",
    "\n",
    "(b) Add or update items in a ToDo list `collection`\n",
    "\n",
    "(c) Update its own `instructions` on how to update items to the ToDo list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "462f25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b415360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateMemory(TypedDict):\n",
    "    \"\"\" Decision on what memory type to update \"\"\"\n",
    "    update_type = Literal['user', 'todo', 'instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c94e8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User profile schema\n",
    "class Profile(BaseModel):\n",
    "    \"\"\"This is the profile of the user you are chatting with\"\"\"\n",
    "\n",
    "    name: Optional[str] = Field(\n",
    "        description=\"The user's name\", default=None\n",
    "    )\n",
    "    location: Optional[str] = Field(\n",
    "        description=\"The user's location\", default=None\n",
    "    )\n",
    "    job: Optional[str] = Field(\n",
    "        description=\"The user's job\", default=None\n",
    "    )\n",
    "    connections: list[str] = Field(\n",
    "        description=\"Personal connection of the user, such as family members, friends, or coworkers\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    interests: list[str] = Field(\n",
    "        description=\"Interests that the user has\", \n",
    "        default_factory=list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f5334c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo schema\n",
    "class ToDo(BaseModel):\n",
    "    task: str = Field(\n",
    "        description=\"The task to be completed.\"\n",
    "    )\n",
    "    time_to_complete: Optional[int] = Field(\n",
    "        description=\"Estimated time to complete the task (minutes).\"\n",
    "    )\n",
    "    deadline: Optional[datetime] = Field(\n",
    "        description=\"When the task needs to be completed by (if applicable)\",\n",
    "        default=None\n",
    "    )\n",
    "    solutions: list[str] = Field(\n",
    "        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n",
    "        max_length=1,\n",
    "        default_factory=list\n",
    "    )\n",
    "    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n",
    "        description=\"Current status of the task\",\n",
    "        default=\"not started\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f2bd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trustcall extractor for updating the user profile \n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084774f",
   "metadata": {},
   "source": [
    "### **Node definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13a7c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_mAIstro(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    \n",
    "    # Retrieve profile memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile = None\n",
    "\n",
    "    # Retrieve task memory from the store\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    # Retrieve custom instructions\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "    \n",
    "    # Respond using memory as well as the chat history\n",
    "    system_msg = [SystemMessage(\n",
    "        MODEL_SYSTEM_MESSAGE.format(\n",
    "            user_profile=user_profile,\n",
    "            todo=todo,\n",
    "            instructions=instructions\n",
    "        )\n",
    "    )]\n",
    "    \n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke(\n",
    "        system_msg + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    return { \"messages\": [response] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37f1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
