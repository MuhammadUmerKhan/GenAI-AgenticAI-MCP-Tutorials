{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7a3a17",
   "metadata": {},
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729a8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia langchain_tavily langchain_groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c135f32",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "007db312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, operator, warnings, getpass\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Send\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from IPython.display import Image, display, Markdown\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from prompt import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923bda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d03be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm.invoke(\"Hello, how are you?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2815d",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42104007",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"research_assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6246de",
   "metadata": {},
   "source": [
    "## **Generate Analysts: Human-In-The-Loop**\n",
    "\n",
    "Create analysts and review them using human-in-the-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d613005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysts(BaseModel):\n",
    "    affiliation: str = Field(..., description=\"Primary affiliation of the analyst.\")\n",
    "    name: str = Field(..., description=\"Name of the analyst.\")\n",
    "    role: str = Field(..., description=\"Role of the analyst in the context of topic.\")\n",
    "    description: str = Field(..., description=\"Description of the analyst focus, concerns, and motives.\")\n",
    "\n",
    "    @property\n",
    "    def personality(self):\n",
    "        return f\"Name: {self.name}, \\nRole: {self.role}, \\nDescription: {self.description} \\nAffiliation: {self.affiliation}\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analyst: List[Analysts] = Field(..., description=\"comprehensive list of analysts with their roles, descriptions, and affiliations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f549944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnalystState(TypedDict):\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    human_analyst_feedback: str\n",
    "    analysts: List[Analysts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8668df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analyst(state: GenerateAnalystState) -> dict:\n",
    "    \"\"\"Create an analyst to generate a report.\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    # Create a structured LLM\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "    \n",
    "    # system prompt\n",
    "    system_prompt = SystemMessage(\n",
    "        content=ANALYST_INSTRUCTIONS.format(\n",
    "            topic=topic,\n",
    "            max_analysts=max_analysts,\n",
    "            human_analyst_feedback=human_analyst_feedback,\n",
    "        ))\n",
    "    \n",
    "    # human message\n",
    "    human_message = HumanMessage(content=\"Create a set of analysts.\")\n",
    "\n",
    "    # create the analyst\n",
    "    analysts = structured_llm.invoke([system_prompt, human_message])\n",
    "\n",
    "    return {\"analysts\": analysts.analyst }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3af61462",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysts = create_analyst({\n",
    "    \"topic\": \"AI Agents\",\n",
    "    \"max_analysts\": 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e67ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Lab\n",
      "Role: AI Ethics Researcher\n",
      "Description: Dr. Carter focuses on the ethical implications of AI agents, exploring how they impact society, privacy, and human rights. She is concerned with ensuring that AI technologies are developed and deployed responsibly, advocating for transparency and accountability in AI systems.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Future of Work Institute\n",
      "Role: AI Workforce Analyst\n",
      "Description: Mr. Liu analyzes the impact of AI agents on the workforce, studying how automation and AI technologies are reshaping job markets and employment trends. He aims to provide insights on how businesses and workers can adapt to the changing landscape brought about by AI advancements.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts.get(\"analysts\"):\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba8344ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback(state: GenerateAnalystState):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    pass\n",
    "\n",
    "def should_continue(state: GenerateAnalystState):\n",
    "    \"\"\" Return the next node to execute \"\"\"\n",
    "    \n",
    "    # Check if human feedback is present\n",
    "    if state.get(\"human_analyst_feedback\", None):\n",
    "        return \"create_analyst\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc6b335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes and edges \n",
    "analyst_builder = StateGraph(GenerateAnalystState)\n",
    "\n",
    "# Add nodes\n",
    "analyst_builder.add_node(\"create_analyst\", create_analyst)\n",
    "analyst_builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# Add edges\n",
    "analyst_builder.add_edge(START, \"create_analyst\")\n",
    "analyst_builder.add_edge(\"create_analyst\", \"human_feedback\")\n",
    "analyst_builder.add_conditional_edges(\"human_feedback\", should_continue, [\"create_analyst\", END])\n",
    "\n",
    "# Compile graph\n",
    "memory = MemorySaver()\n",
    "analyst_graph = analyst_builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0a368f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View\n",
    "# display(Image(analyst_graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "# analyst_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e449af",
   "metadata": {},
   "source": [
    "### **Test the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c24b4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Inc.\n",
      "Role: AI Framework Specialist\n",
      "Description: Dr. Carter focuses on the technical advantages of adopting LangGraph, emphasizing its modular architecture and ease of integration with existing systems. She is particularly interested in how LangGraph can enhance the efficiency of AI agents in various applications.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Future AI Research Group\n",
      "Role: AI Ethics Analyst\n",
      "Description: Mr. Liu examines the ethical implications of using LangGraph as an agent framework. He is concerned with issues such as data privacy, bias in AI decision-making, and the transparency of AI processes, advocating for responsible AI development.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name: Ms. Sarah Thompson\n",
      "Affiliation: Business Solutions Consultancy\n",
      "Role: Business Strategy Consultant\n",
      "Description: Ms. Thompson analyzes the business benefits of adopting LangGraph, focusing on cost-effectiveness, scalability, and the potential for improved customer engagement. She aims to provide insights on how businesses can leverage LangGraph to gain a competitive edge.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "max_analysts = 3 \n",
    "topic = \"The benefits of adopting LangGraph as an agent framework\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "input_state = { \"topic\": topic, \"max_analysts\": max_analysts }\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in analyst_graph.stream(input_state, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1db6b173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state and look at next node\n",
    "current_state = analyst_graph.get_state(thread)\n",
    "current_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fde348cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Node--\n",
      "human_feedback\n"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in analyst_graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2b88c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0e7d1f-9cdf-60ca-8003-b51511c63d67'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are satisfied, then we simply supply no feedback\n",
    "further_feedback = None\n",
    "analyst_graph.update_state(\n",
    "    thread, \n",
    "    { \"human_analyst_feedback\": further_feedback}, as_node=\"human_feedback\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7db6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in analyst_graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85467b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = analyst_graph.get_state(thread)\n",
    "analysts = final_state.values.get(\"analysts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7eb0601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00a44e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Inc.\n",
      "Role: AI Framework Specialist\n",
      "Description: Dr. Carter focuses on the technical advantages of adopting LangGraph, emphasizing its modular architecture and ease of integration with existing systems. She is particularly interested in how LangGraph can enhance the efficiency of AI agents in various applications.\n",
      "--------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Future AI Research Group\n",
      "Role: AI Ethics Analyst\n",
      "Description: Mr. Liu examines the ethical implications of using LangGraph as an agent framework. He is concerned with issues such as data privacy, bias in AI decision-making, and the transparency of AI processes, advocating for responsible AI development.\n",
      "--------------------------------------------------\n",
      "Name: Ms. Sarah Thompson\n",
      "Affiliation: Business Solutions Consultancy\n",
      "Role: Business Strategy Consultant\n",
      "Description: Ms. Thompson analyzes the business benefits of adopting LangGraph, focusing on cost-effectiveness, scalability, and the potential for improved customer engagement. She aims to provide insights on how businesses can leverage LangGraph to gain a competitive edge.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba28cf0",
   "metadata": {},
   "source": [
    "## **Conduct Interview**\n",
    "\n",
    "### **Generate Question**\n",
    "\n",
    "- **The analyst will ask questions to the expert.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "535c44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]    # Messages\n",
    "    max_num_turns: int                                      # Number turns of conversation\n",
    "    context: Annotated[list, add_messages]                  # Source docs\n",
    "    analyst: Analysts                                       # Analyst asking questions\n",
    "    interview: str                                          # Interview transcript\n",
    "    section: str                                            # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "    search_query: Annotated[List, add_messages]\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f604f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(state: InterviewState) -> dict:\n",
    "    \"\"\" Generate question for analyst. \"\"\"\n",
    "    \n",
    "    analyst = state['analyst']\n",
    "    messages = state['messages']\n",
    "\n",
    "    # generate question\n",
    "    system_message = SystemMessage(content=QUESTION_INSTRUCTIONS.format(goals=analyst.persona))\n",
    "    question = llm.invoke([system_message] + messages)\n",
    "\n",
    "    # write message to state\n",
    "    return { \"messages\": [question] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29d30224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Ms. Sarah Thompson, \\nRole: Business Strategy Consultant, \\nDescription: Ms. Thompson analyzes the business benefits of adopting LangGraph, focusing on cost-effectiveness, scalability, and the potential for improved customer engagement. She aims to provide insights on how businesses can leverage LangGraph to gain a competitive edge. \\nAffiliation: Business Solutions Consultancy'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in above node change \"analyst.persona -> analyst['persona']\" for testing below analyst persona\n",
    "analyst_persona = {\n",
    "    \"persona\":\n",
    "    f\"Name: {analyst.name}, \\nRole: {analyst.role}, \\nDescription: {analyst.description} \\nAffiliation: {analyst.affiliation}\"\n",
    "    }\n",
    "analyst_persona['persona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47023f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = generate_question({\n",
    "    \"analyst\": analyst_persona,\n",
    "    \"messages\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3bad88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Ms. Thompson, my name is Alex Carter, and Iâ€™m an analyst focused on understanding innovative business strategies. Iâ€™m excited to speak with you today about LangGraph and its potential benefits for businesses. \\n\\nTo start, could you share some specific examples of how adopting LangGraph has led to cost savings for companies? What are some surprising ways that businesses have found to reduce expenses through its implementation?'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question['messages'][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3469ae",
   "metadata": {},
   "source": [
    "## **Generate Answer: Parallelization**\n",
    "The expert will gather information from multiple sources in parallel to answer questions.\n",
    "\n",
    "For example, we can use:\n",
    "\n",
    "- Specific web sites e.g., via WebBaseLoader\n",
    "- Indexed documents e.g., via [RAG](https://docs.langchain.com/oss/python/langchain/retrieval)\n",
    "- Web search\n",
    "- Wikipedia search\n",
    "\n",
    "You can try different web search tools, like [Tavily](https://www.tavily.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c8c3a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9e038",
   "metadata": {},
   "source": [
    "Now, we create nodes to search the web and wikipedia.\n",
    "\n",
    "We'll also create a node to answer analyst questions.\n",
    "\n",
    "Finally, we'll create nodes to save the full interview and to write a summary (\"section\") of the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a90a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state: InterviewState) -> dict:\n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "    \n",
    "    message = state['messages']\n",
    "    \n",
    "    # search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke(\n",
    "        [SEARCH_INSTRUCTIONS] + message\n",
    "    )\n",
    "\n",
    "    # search\n",
    "    data = tavily_search.invoke({\"query\": search_query.search_query})\n",
    "    search_docs = data.get(\"results\", data)\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write to state\n",
    "    return { \"context\": [formatted_search_docs], \"search_query\": [search_query] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "04e60854",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_result = web_search({\"messages\": question[\"messages\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16f3bf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "cost savings examples LangGraph implementation businesses surprising ways reduce expenses"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(web_search_result['search_query'][0].search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57c7ec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<Document href=\"https://www.youtube.com/watch?v=9RFNOYtkwsc\"/>\n",
       "How Prosper Cut QA Costs by 90% for Financial Services with LangGraph Agents\n",
       "LangChain\n",
       "164000 subscribers\n",
       "79 likes\n",
       "4411 views\n",
       "1 Jul 2025\n",
       "Learn how Prosper Marketplace transformed their customer call QA process for financial services using LangGraph, reducing verification costs from tens of dollars to cents per call. In this video, see how their AI team built a flexible agent platform that now verifies 100% of customer calls automatically, replacing expensive manual processes.\n",
       "\n",
       "Key highlights:\n",
       "\n",
       "- 90%+ cost reduction in QA verification\n",
       "- From sample-based to 100% call verification automatically\n",
       "- LangGraph checkpoint system for rapid iteration\n",
       "- Human-in-the-loop workflows with interrupts\n",
       "- Scalable multi-agent architecture\n",
       "\n",
       "ðŸ”— Discover more agent engineer stories: langchain.com/customers\n",
       "4 comments\n",
       "\n",
       "</Document>\n",
       "\n",
       "---\n",
       "\n",
       "<Document href=\"https://www.metacto.com/blogs/langgraph-pricing-explained-a-deep-dive-into-integration-maintenance-costs\"/>\n",
       "This guide offers a detailed look at the financial and technical investments required for leveraging LangGraph in your AI-powered\n",
       "</Document>\n",
       "\n",
       "---\n",
       "\n",
       "<Document href=\"https://rasa.com/blog/cutting-ai-assistant-costs-the-power-of-enhancing-llms-with-business\"/>\n",
       "While studying different approaches, we found that semi-structured approaches separating conversational ability from business logic execution strike the necessary balance, providing reliable and consistent results without sacrificing flexibility. For instance, across our experiments, the LangGraph assistant incurs a mean cost of $0.10 per user message, more than 2 times the CALM Assistantâ€™s mean cost of $0.04. the LangGraph approach lacked consistency and reliability in the conversations, interactions, and execution of required tasks (take a look at the screenshots in this post to get a sense of these mistakes). All conversations with the CALM assistant will move through the business logic as above and will differ only in the type of chit-chat and rephrasing necessary to make the conversation more natural. To verify this across all conversations, we have built end-to-end tests that make sure the business logic is followed and the CALM Assistant passes all such tests. 1. Approaches that separate conversational ability from the execution of business logic, such as CALM, have a significant edge in terms of response time and operational costs.\n",
       "</Document>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(web_search_result['context'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b88fefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(state: InterviewState) -> dict:\n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "    \n",
    "    message = state['messages']\n",
    "    \n",
    "    # search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke(\n",
    "        [SEARCH_INSTRUCTIONS] + message\n",
    "    )\n",
    "\n",
    "    # search\n",
    "    search_docs = WikipediaLoader(query=search_query.search_query, load_max_docs=2).load()\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write to state\n",
    "    return { \"context\": [formatted_search_docs], \"search_query\": [search_query] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f39b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_wikipedia_result = search_wikipedia({\n",
    "    \"messages\": question[\"messages\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914bf47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
